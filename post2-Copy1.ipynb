{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('mercari-price-suggestion-challenge/train.tsv', sep='\\t')\n",
    "df_train, df_test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1111901, 8), (370634, 8))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1323343</th>\n",
       "      <td>1323343</td>\n",
       "      <td>FAST FREE SHIP Love pink lanyard</td>\n",
       "      <td>1</td>\n",
       "      <td>Handmade/Accessories/Lanyard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This lanyard fits comfortably around the neck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130463</th>\n",
       "      <td>1130463</td>\n",
       "      <td>Disney Aladdin Blu Ray Disc ONLY</td>\n",
       "      <td>2</td>\n",
       "      <td>Electronics/Media/Blu-Ray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Perfect condition Disney Aladdin Blu Ray Disc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429519</th>\n",
       "      <td>1429519</td>\n",
       "      <td>Cleansing detox foot pads</td>\n",
       "      <td>1</td>\n",
       "      <td>Other/Daily &amp; Travel items/Health Care</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New 3 packs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29219</th>\n",
       "      <td>29219</td>\n",
       "      <td>FLAWLESS iPhone 6 , Verizon 16GB</td>\n",
       "      <td>2</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Cell Pho...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Only used this phone for about 8 months, basic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545342</th>\n",
       "      <td>545342</td>\n",
       "      <td>Womens sz xl / 16 lot dresses top skirts</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Dresses/Knee-Length</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8 dresses. 3 skirts. 1 top.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                                      name  \\\n",
       "1323343   1323343          FAST FREE SHIP Love pink lanyard   \n",
       "1130463   1130463          Disney Aladdin Blu Ray Disc ONLY   \n",
       "1429519   1429519                 Cleansing detox foot pads   \n",
       "29219       29219          FLAWLESS iPhone 6 , Verizon 16GB   \n",
       "545342     545342  Womens sz xl / 16 lot dresses top skirts   \n",
       "\n",
       "         item_condition_id                                      category_name  \\\n",
       "1323343                  1                       Handmade/Accessories/Lanyard   \n",
       "1130463                  2                          Electronics/Media/Blu-Ray   \n",
       "1429519                  1             Other/Daily & Travel items/Health Care   \n",
       "29219                    2  Electronics/Cell Phones & Accessories/Cell Pho...   \n",
       "545342                   3                          Women/Dresses/Knee-Length   \n",
       "\n",
       "        brand_name  price  shipping  \\\n",
       "1323343        NaN    6.0         1   \n",
       "1130463        NaN    6.0         1   \n",
       "1429519        NaN   16.0         1   \n",
       "29219        Apple  200.0         1   \n",
       "545342         NaN   50.0         1   \n",
       "\n",
       "                                          item_description  \n",
       "1323343  This lanyard fits comfortably around the neck ...  \n",
       "1130463  Perfect condition Disney Aladdin Blu Ray Disc ...  \n",
       "1429519                                       New 3 packs.  \n",
       "29219    Only used this phone for about 8 months, basic...  \n",
       "545342                         8 dresses. 3 skirts. 1 top.  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.111901e+06</td>\n",
       "      <td>1111901</td>\n",
       "      <td>1.111901e+06</td>\n",
       "      <td>1107111</td>\n",
       "      <td>637123</td>\n",
       "      <td>1.111901e+06</td>\n",
       "      <td>1.111901e+06</td>\n",
       "      <td>1111897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>936651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1266</td>\n",
       "      <td>4450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>967793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bundle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Women/Athletic Apparel/Pants, Tights, Leggings</td>\n",
       "      <td>PINK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45126</td>\n",
       "      <td>40599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.411639e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.907733e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.674282e+01</td>\n",
       "      <td>4.471270e-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.280626e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.032344e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.864513e+01</td>\n",
       "      <td>4.971968e-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.702290e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.414630e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.111936e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.482534e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.006000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_id     name  item_condition_id  \\\n",
       "count   1.111901e+06  1111901       1.111901e+06   \n",
       "unique           NaN   936651                NaN   \n",
       "top              NaN   Bundle                NaN   \n",
       "freq             NaN     1715                NaN   \n",
       "mean    7.411639e+05      NaN       1.907733e+00   \n",
       "std     4.280626e+05      NaN       9.032344e-01   \n",
       "min     0.000000e+00      NaN       1.000000e+00   \n",
       "25%     3.702290e+05      NaN       1.000000e+00   \n",
       "50%     7.414630e+05      NaN       2.000000e+00   \n",
       "75%     1.111936e+06      NaN       3.000000e+00   \n",
       "max     1.482534e+06      NaN       5.000000e+00   \n",
       "\n",
       "                                         category_name brand_name  \\\n",
       "count                                          1107111     637123   \n",
       "unique                                            1266       4450   \n",
       "top     Women/Athletic Apparel/Pants, Tights, Leggings       PINK   \n",
       "freq                                             45126      40599   \n",
       "mean                                               NaN        NaN   \n",
       "std                                                NaN        NaN   \n",
       "min                                                NaN        NaN   \n",
       "25%                                                NaN        NaN   \n",
       "50%                                                NaN        NaN   \n",
       "75%                                                NaN        NaN   \n",
       "max                                                NaN        NaN   \n",
       "\n",
       "               price      shipping    item_description  \n",
       "count   1.111901e+06  1.111901e+06             1111897  \n",
       "unique           NaN           NaN              967793  \n",
       "top              NaN           NaN  No description yet  \n",
       "freq             NaN           NaN               61890  \n",
       "mean    2.674282e+01  4.471270e-01                 NaN  \n",
       "std     3.864513e+01  4.971968e-01                 NaN  \n",
       "min     0.000000e+00  0.000000e+00                 NaN  \n",
       "25%     1.000000e+01  0.000000e+00                 NaN  \n",
       "50%     1.700000e+01  0.000000e+00                 NaN  \n",
       "75%     2.900000e+01  1.000000e+00                 NaN  \n",
       "max     2.006000e+03  1.000000e+00                 NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgates/venvs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/dgates/venvs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/dgates/venvs/py3/lib/python3.6/site-packages/pandas/core/frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/dgates/venvs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/dgates/venvs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "def preprocess(df):\n",
    "    category_ids = {v: i for i, v in enumerate(df.category_name.unique(), start=1)}\n",
    "    brand_ids = {v: i for i, v in enumerate(df.brand_name.unique(), start=1)}\n",
    "    df['category_id'] = df.category_name.map(category_ids)\n",
    "    df['brand_id'] = df.brand_name.map(brand_ids)\n",
    "    \n",
    "    df[['category_id', 'brand_id', 'item_condition_id']].fillna(0, inplace=True)\n",
    "    df['text'] = df.name + ' ' + df.item_description.str.replace('No description yet', '')\n",
    "    df['text'] = df.text.astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_test = preprocess(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.111901e+06\n",
       "mean     2.895952e+01\n",
       "std      3.085213e+01\n",
       "min      0.000000e+00\n",
       "25%      1.000000e+01\n",
       "50%      1.900000e+01\n",
       "75%      3.500000e+01\n",
       "max      2.510000e+02\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.text.str.count(' ').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>category_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1323343</th>\n",
       "      <td>1323343</td>\n",
       "      <td>FAST FREE SHIP Love pink lanyard</td>\n",
       "      <td>1</td>\n",
       "      <td>Handmade/Accessories/Lanyard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This lanyard fits comfortably around the neck ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FAST FREE SHIP Love pink lanyard This lanyard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130463</th>\n",
       "      <td>1130463</td>\n",
       "      <td>Disney Aladdin Blu Ray Disc ONLY</td>\n",
       "      <td>2</td>\n",
       "      <td>Electronics/Media/Blu-Ray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Perfect condition Disney Aladdin Blu Ray Disc ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Disney Aladdin Blu Ray Disc ONLY Perfect condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429519</th>\n",
       "      <td>1429519</td>\n",
       "      <td>Cleansing detox foot pads</td>\n",
       "      <td>1</td>\n",
       "      <td>Other/Daily &amp; Travel items/Health Care</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New 3 packs.</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Cleansing detox foot pads New 3 packs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29219</th>\n",
       "      <td>29219</td>\n",
       "      <td>FLAWLESS iPhone 6 , Verizon 16GB</td>\n",
       "      <td>2</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Cell Pho...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Only used this phone for about 8 months, basic...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>FLAWLESS iPhone 6 , Verizon 16GB Only used thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545342</th>\n",
       "      <td>545342</td>\n",
       "      <td>Womens sz xl / 16 lot dresses top skirts</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Dresses/Knee-Length</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8 dresses. 3 skirts. 1 top.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Womens sz xl / 16 lot dresses top skirts 8 dre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                                      name  \\\n",
       "1323343   1323343          FAST FREE SHIP Love pink lanyard   \n",
       "1130463   1130463          Disney Aladdin Blu Ray Disc ONLY   \n",
       "1429519   1429519                 Cleansing detox foot pads   \n",
       "29219       29219          FLAWLESS iPhone 6 , Verizon 16GB   \n",
       "545342     545342  Womens sz xl / 16 lot dresses top skirts   \n",
       "\n",
       "         item_condition_id                                      category_name  \\\n",
       "1323343                  1                       Handmade/Accessories/Lanyard   \n",
       "1130463                  2                          Electronics/Media/Blu-Ray   \n",
       "1429519                  1             Other/Daily & Travel items/Health Care   \n",
       "29219                    2  Electronics/Cell Phones & Accessories/Cell Pho...   \n",
       "545342                   3                          Women/Dresses/Knee-Length   \n",
       "\n",
       "        brand_name  price  shipping  \\\n",
       "1323343        NaN    6.0         1   \n",
       "1130463        NaN    6.0         1   \n",
       "1429519        NaN   16.0         1   \n",
       "29219        Apple  200.0         1   \n",
       "545342         NaN   50.0         1   \n",
       "\n",
       "                                          item_description  category_id  \\\n",
       "1323343  This lanyard fits comfortably around the neck ...            1   \n",
       "1130463  Perfect condition Disney Aladdin Blu Ray Disc ...            2   \n",
       "1429519                                       New 3 packs.            3   \n",
       "29219    Only used this phone for about 8 months, basic...            4   \n",
       "545342                         8 dresses. 3 skirts. 1 top.            5   \n",
       "\n",
       "         brand_id                                               text  \n",
       "1323343         1  FAST FREE SHIP Love pink lanyard This lanyard ...  \n",
       "1130463         1  Disney Aladdin Blu Ray Disc ONLY Perfect condi...  \n",
       "1429519         1             Cleansing detox foot pads New 3 packs.  \n",
       "29219           2  FLAWLESS iPhone 6 , Verizon 16GB Only used thi...  \n",
       "545342          1  Womens sz xl / 16 lot dresses top skirts 8 dre...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build byte pair encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent=None):\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.index = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Node(index={self.index}, children={self.children})'\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        return self.children.get(key, default)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.children[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.children[key] = value\n",
    "        \n",
    "    def __contains__(self, key):\n",
    "        return key in self.children\n",
    "\n",
    "\n",
    "def build_bpe_tree(vocab):\n",
    "    root = Node()\n",
    "    for word, index in vocab.items():\n",
    "        current_node = root\n",
    "        for n, c in enumerate(word, start=1):\n",
    "            if not c in current_node:\n",
    "                current_node[c] = Node()\n",
    "            current_node = current_node[c]\n",
    "            if n == len(word):\n",
    "                current_node.index = index\n",
    "    return root\n",
    "\n",
    "\n",
    "def apply_bpe_tree(text, tree):\n",
    "    output = []\n",
    "    pos = 0\n",
    "    last_node = tree\n",
    "    while pos <= len(text) - 1:\n",
    "        node = last_node.get(text[pos])\n",
    "        # we can't search the tree any further\n",
    "        if node is None:\n",
    "            # we couldn't search the tree any further but we\n",
    "            # ended up at a node that doesn't correspond to a\n",
    "            # word in the learned vocabulary.\n",
    "            # In this case we'll traverse back through the tree\n",
    "            # until we hit a node with an index.\n",
    "            if last_node.index is None:\n",
    "                while last_node.index is not None:\n",
    "                    last_node = last_node.parent\n",
    "                    pos -= 1\n",
    "            # add the last seen index to the output\n",
    "            # and reset variables for next run through\n",
    "            output.append(last_node.index)\n",
    "            if last_node is not tree:\n",
    "                last_node = tree\n",
    "                continue\n",
    "            node = tree\n",
    "        last_node = node\n",
    "        pos += 1\n",
    "    output.append(last_node.index)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def get_stats(list vocab):\n",
    "    cdef c1, c2\n",
    "    cdef list word\n",
    "    cdef int freq, vocab_pos, word_pos\n",
    "    cdef dict pair_stats = {}\n",
    "    cdef dict pair_indices = {}\n",
    "    cdef tuple pair\n",
    "    for vocab_pos in range(len(vocab)):\n",
    "        word, freq = vocab[vocab_pos]\n",
    "        for word_pos in range(len(word) - 1):\n",
    "            pair = word[word_pos], word[word_pos + 1]\n",
    "            if not pair in pair_stats:\n",
    "                pair_stats[pair] = 0\n",
    "            pair_stats[pair] += freq\n",
    "            if not pair in pair_indices:\n",
    "                pair_indices[pair] = []\n",
    "            pair_indices[pair].append((vocab_pos, word_pos))\n",
    "    return pair_stats, pair_indices\n",
    "\n",
    "\n",
    "def merge_vocab(tuple pair, list vocab, list pair_indices):\n",
    "    cdef int vocab_pos, word_pos\n",
    "    cdef list word\n",
    "    for vocab_pos, word_pos in reversed(pair_indices):\n",
    "        word, _ = vocab[vocab_pos]\n",
    "        word[word_pos] = word[word_pos] + word[word_pos + 1]\n",
    "        word.pop(word_pos + 1)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BytePairEncoder(sklearn.base.TransformerMixin):    \n",
    "    _unkown_character = '<unk>'\n",
    "    _space_escape = '▁'\n",
    "\n",
    "    def __init__(self, target_vocab_size, vocab_threshold=None, log_level=None):\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.log_level = log_level\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "\n",
    "        # these will all be set during .fit()\n",
    "        self.vocab = None\n",
    "        self._vocab_stats = None\n",
    "        self._reverse_vocab = None\n",
    "        self._bpe_tree = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # get the initial vocabular consisting of all unique characters\n",
    "        initial_vocab = set(X)\n",
    "        initial_vocab.add(self._space_escape)\n",
    "\n",
    "        words = self._split_X(X)\n",
    "        vocab = [(list(word), freq) for word, freq in collections.Counter(words).items()]\n",
    "\n",
    "        t_started = time.time()\n",
    "        i = 0\n",
    "        while self._vocab_size(vocab) < self.target_vocab_size:\n",
    "            if self.log_level is not None \\\n",
    "                    and i and (i + 1) % self.log_level == 0:\n",
    "                print(f'{i+1} iterations complete in {time.time() - t_started}')\n",
    "            pair_stats, pair_index = get_stats(vocab)\n",
    "            best = max(pair_stats, key=pair_stats.get)\n",
    "            if self.vocab_threshold is not None \\\n",
    "                    and pair_stats[best] < self.vocab_threshold:\n",
    "                print(f'Stopping after {i} iterations. Best pair occurs '\n",
    "                      f'{pair_stats[best]} < {self.vocab_threshold} times')\n",
    "                break\n",
    "            vocab = merge_vocab(best, vocab, pair_index[best])\n",
    "            i += 1\n",
    "\n",
    "        # build the final vocabulary\n",
    "        vocab_stats = collections.Counter()\n",
    "        _ = [vocab_stats.update({subword: freq})\n",
    "                                for word, freq in vocab\n",
    "                                for subword in word]\n",
    "        final_vocab = set(vocab_stats)\n",
    "        final_vocab.update(initial_vocab)\n",
    "        final_vocab = {k: i for i, k in enumerate(final_vocab, start=1)}\n",
    "        final_vocab[self._unkown_character] = 0\n",
    "        self.vocab = final_vocab\n",
    "\n",
    "        # these are needed for .transform() and .inverse_transform()\n",
    "        self._reverse_vocab = {i: k for k, i in self.vocab.items()}\n",
    "        self._bpe_tree = build_bpe_tree(self.vocab)\n",
    "\n",
    "        # keep this for curiosity/debugging\n",
    "        self._vocab_stats = vocab_stats\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._split_X(X)\n",
    "        return np.concatenate([self._transform_string(x) for x in X])\n",
    "\n",
    "    def _transform_string(self, X):\n",
    "        tokens = apply_bpe_tree(X, self._bpe_tree)\n",
    "        return np.array([0 if t is None else t for t in tokens])                \n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [self._reverse_vocab[t] for t in X]\n",
    "\n",
    "    def _split_X(self, X):\n",
    "        return [word + self._space_escape for word in X.split()]\n",
    "\n",
    "    def _vocab_size(self, vocab):\n",
    "        return len(set(subword for word, _ in vocab for subword in word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hollister jean shorts, SIZE 9. Great condition. prices are NOT firm check my closet and save on bundles F21 (M) thigh length knitted New with tag No B'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_text = ' '.join(df_train.item_description.sample(30000))\n",
    "bpe_text[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4416571"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 iterations complete in 262.1924641132355\n",
      "2001 iterations complete in 510.85629892349243\n",
      "3001 iterations complete in 765.7708230018616\n",
      "4001 iterations complete in 1002.2984910011292\n",
      "5001 iterations complete in 1247.4365611076355\n",
      "6001 iterations complete in 1493.677533864975\n",
      "7001 iterations complete in 1748.0518581867218\n",
      "8001 iterations complete in 1978.9800019264221\n",
      "9001 iterations complete in 2218.366352081299\n",
      "10001 iterations complete in 2434.7551760673523\n",
      "11001 iterations complete in 2643.507613182068\n",
      "12001 iterations complete in 2842.9302089214325\n",
      "13001 iterations complete in 3038.519464969635\n",
      "14001 iterations complete in 3230.526780128479\n",
      "15001 iterations complete in 3419.908914089203\n",
      "16001 iterations complete in 3605.580456972122\n",
      "17001 iterations complete in 3788.129462957382\n",
      "18001 iterations complete in 3966.83597612381\n",
      "Stopping after 18771 iterations. Best pair occurs 4 < 5 times\n",
      "CPU times: user 1h 7min 24s, sys: 35.4 s, total: 1h 8min\n",
      "Wall time: 1h 8min 22s\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoder(30000, log_level=1000, vocab_threshold=5)\n",
    "%time bpe.fit(bpe_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18814"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'photos▁': 1,\n",
       " 'sky': 2,\n",
       " 'a14': 3,\n",
       " 'towel.▁': 4,\n",
       " 'photos,▁': 5,\n",
       " 'generation▁': 6,\n",
       " 'oce': 7,\n",
       " 'insp': 8,\n",
       " 'acc': 9,\n",
       " '⌛': 10,\n",
       " 'nur': 11,\n",
       " 'available,▁': 12,\n",
       " '2/': 13,\n",
       " 'latex': 14,\n",
       " 'fric': 15,\n",
       " 'flare▁': 16,\n",
       " '2pc▁': 17,\n",
       " 'pockets.▁': 18,\n",
       " 'shampoo,▁': 19,\n",
       " 'twin▁': 20,\n",
       " 'cambogia▁': 21,\n",
       " 'big': 22,\n",
       " '3-': 23,\n",
       " 'approved!▁': 24,\n",
       " 'cut.▁': 25,\n",
       " 'clin': 26,\n",
       " 'elmers▁': 27,\n",
       " 'liber': 28,\n",
       " 'rage▁': 29,\n",
       " '16-': 30,\n",
       " 'rap▁': 31,\n",
       " '(as▁': 32,\n",
       " 'negotiate!▁': 33,\n",
       " 'digital▁': 34,\n",
       " 'ats▁': 35,\n",
       " 'ovals▁': 36,\n",
       " 'germany▁': 37,\n",
       " 'bowl▁': 38,\n",
       " 'wave▁': 39,\n",
       " 'cord,▁': 40,\n",
       " 'cigare': 41,\n",
       " 'calls▁': 42,\n",
       " 'heel': 43,\n",
       " 'appe': 44,\n",
       " '38d▁': 45,\n",
       " 'batter': 46,\n",
       " 'age': 47,\n",
       " 'shooting▁': 48,\n",
       " 'ped,▁': 49,\n",
       " 'innov': 50,\n",
       " 'team▁': 51,\n",
       " 'eyebrows▁': 52,\n",
       " 'pulled▁': 53,\n",
       " 'bnib▁': 54,\n",
       " 'vines,▁': 55,\n",
       " 'straps.▁': 56,\n",
       " 'gopro▁': 57,\n",
       " 'gk▁': 58,\n",
       " '1.7▁': 59,\n",
       " 'blood▁': 60,\n",
       " 'authenticity▁': 61,\n",
       " 'pted▁': 62,\n",
       " 'peeling▁': 63,\n",
       " 'chero': 64,\n",
       " '☀': 65,\n",
       " '¤▁': 66,\n",
       " 'geometr': 67,\n",
       " 'headphone▁': 68,\n",
       " 'copper▁': 69,\n",
       " 'ness,▁': 70,\n",
       " 'backup▁': 71,\n",
       " 'transforms▁': 72,\n",
       " 'glued▁': 73,\n",
       " 'violet,▁': 74,\n",
       " 'groo': 75,\n",
       " '11.5\"▁': 76,\n",
       " 'a\"▁': 77,\n",
       " 'reten': 78,\n",
       " 'taylor▁': 79,\n",
       " 'moms▁': 80,\n",
       " 'selling.▁': 81,\n",
       " 'disru': 82,\n",
       " '9.00▁': 83,\n",
       " 'madagas': 84,\n",
       " 'value▁': 85,\n",
       " 'aren': 86,\n",
       " 'spinning▁': 87,\n",
       " 'cup,▁': 88,\n",
       " 'tumbler,▁': 89,\n",
       " 'headphones▁': 90,\n",
       " 'mug,▁': 91,\n",
       " '35': 92,\n",
       " 'morphe▁': 93,\n",
       " 'wrang': 94,\n",
       " 'quad': 95,\n",
       " 'leo': 96,\n",
       " 'earpod▁': 97,\n",
       " 'cologne▁': 98,\n",
       " 'upgrad': 99,\n",
       " 'convention▁': 100,\n",
       " 'somewhere▁': 101,\n",
       " 'oz..▁': 102,\n",
       " 'est!▁': 103,\n",
       " 'ely▁': 104,\n",
       " 'p!▁': 105,\n",
       " 'weird▁': 106,\n",
       " '49': 107,\n",
       " 'boy▁': 108,\n",
       " 'same/next▁': 109,\n",
       " 'elastane.▁': 110,\n",
       " 'discount.▁': 111,\n",
       " 'everyone▁': 112,\n",
       " 'waistband▁': 113,\n",
       " 'month': 114,\n",
       " 'daily.▁': 115,\n",
       " 'ify▁': 116,\n",
       " 'monroe▁': 117,\n",
       " 'ure▁': 118,\n",
       " 'blue:▁': 119,\n",
       " '[rm]+': 120,\n",
       " 'eland▁': 121,\n",
       " '4.5\"▁': 122,\n",
       " 'dallas▁': 123,\n",
       " 'pallet▁': 124,\n",
       " 'he': 125,\n",
       " 'stop▁': 126,\n",
       " 'absorption▁': 127,\n",
       " 'piece▁': 128,\n",
       " 'stone': 129,\n",
       " 'who': 130,\n",
       " 'very▁': 131,\n",
       " 'al': 132,\n",
       " 'a': 133,\n",
       " 'bearing▁': 134,\n",
       " 'hottest▁': 135,\n",
       " 'torrid▁': 136,\n",
       " 'bud': 137,\n",
       " 'firm/no▁': 138,\n",
       " 'opy▁': 139,\n",
       " 'femin': 140,\n",
       " 'tic': 141,\n",
       " 'firm!▁': 142,\n",
       " 'ying▁': 143,\n",
       " 'bootie▁': 144,\n",
       " 'enables▁': 145,\n",
       " 'body.▁': 146,\n",
       " 'ff▁': 147,\n",
       " 'silicone▁': 148,\n",
       " 'me:▁': 149,\n",
       " 'mic▁': 150,\n",
       " 'sleeve.▁': 151,\n",
       " 'pit': 152,\n",
       " 'gum': 153,\n",
       " 'samsung▁': 154,\n",
       " 'totor': 155,\n",
       " 'wrinkled▁': 156,\n",
       " 'bio': 157,\n",
       " 'smoke-free,▁': 158,\n",
       " 'nor': 159,\n",
       " 'whiten': 160,\n",
       " 'kle': 161,\n",
       " 'warmer': 162,\n",
       " 'herbal▁': 163,\n",
       " '14g▁': 164,\n",
       " '~~~~': 165,\n",
       " 'intense▁': 166,\n",
       " 'muffin▁': 167,\n",
       " 'dir': 168,\n",
       " 'anything.▁': 169,\n",
       " 'feeding▁': 170,\n",
       " 'formula▁': 171,\n",
       " 'regular.▁': 172,\n",
       " '+tax▁': 173,\n",
       " 'bunch▁': 174,\n",
       " 'cow': 175,\n",
       " '-comes▁': 176,\n",
       " 'lipgloss▁': 177,\n",
       " 'help▁': 178,\n",
       " 'an': 179,\n",
       " 'slim▁': 180,\n",
       " 'scen': 181,\n",
       " 'yankee▁': 182,\n",
       " '2,': 183,\n",
       " 'revolu': 184,\n",
       " 'stopper▁': 185,\n",
       " 'kim': 186,\n",
       " 'socks.▁': 187,\n",
       " 'kes,▁': 188,\n",
       " 'reviews▁': 189,\n",
       " 'projec': 190,\n",
       " '16.9▁': 191,\n",
       " 'oy▁': 192,\n",
       " 'ex': 193,\n",
       " 'jasmine,▁': 194,\n",
       " 'explore': 195,\n",
       " 'oleophobic▁': 196,\n",
       " 'chet▁': 197,\n",
       " 'cases,▁': 198,\n",
       " 'decoration': 199,\n",
       " 'rewind▁': 200,\n",
       " 'confetti▁': 201,\n",
       " 'jade▁': 202,\n",
       " 'asonic▁': 203,\n",
       " 'os,▁': 204,\n",
       " 'ka-': 205,\n",
       " 'polly▁': 206,\n",
       " 'bag!▁': 207,\n",
       " 'msg▁': 208,\n",
       " 'auxiliary▁': 209,\n",
       " 'disco': 210,\n",
       " 'jo▁': 211,\n",
       " 'ane': 212,\n",
       " 'swim': 213,\n",
       " 'mex': 214,\n",
       " 'otter': 215,\n",
       " 'cola▁': 216,\n",
       " 'pirates▁': 217,\n",
       " 'some': 218,\n",
       " 'ly,▁': 219,\n",
       " 'packets▁': 220,\n",
       " 'lac': 221,\n",
       " 'holster▁': 222,\n",
       " 'compu': 223,\n",
       " 'rai': 224,\n",
       " '-up.▁': 225,\n",
       " 'val▁': 226,\n",
       " 'hoo': 227,\n",
       " 'covered▁': 228,\n",
       " 'metr': 229,\n",
       " 'laptops▁': 230,\n",
       " 'daily,▁': 231,\n",
       " 'school,▁': 232,\n",
       " 'harm▁': 233,\n",
       " 'hiking,▁': 234,\n",
       " '•for▁': 235,\n",
       " 'only!▁': 236,\n",
       " 'tomorrow▁': 237,\n",
       " 'fau': 238,\n",
       " 'ch': 239,\n",
       " 'nhl▁': 240,\n",
       " '(l': 241,\n",
       " 'vide': 242,\n",
       " 'prescription▁': 243,\n",
       " 'fl.': 244,\n",
       " 'so.▁': 245,\n",
       " '...........': 246,\n",
       " 'bows▁': 247,\n",
       " 'seam▁': 248,\n",
       " 'use,▁': 249,\n",
       " 'condition,▁': 250,\n",
       " 'aden▁': 251,\n",
       " '❓': 252,\n",
       " 'janu': 253,\n",
       " 'meal▁': 254,\n",
       " 'te': 255,\n",
       " '-hard▁': 256,\n",
       " \"ry's▁\": 257,\n",
       " 'buyer.▁': 258,\n",
       " 'je': 259,\n",
       " 'kit:▁': 260,\n",
       " 'bug▁': 261,\n",
       " 'elegance▁': 262,\n",
       " 'music': 263,\n",
       " 'coupons▁': 264,\n",
       " '☺️▁': 265,\n",
       " 'tote▁': 266,\n",
       " 'ale,▁': 267,\n",
       " 'next▁': 268,\n",
       " 'ings)▁': 269,\n",
       " 'lemon': 270,\n",
       " 'solo▁': 271,\n",
       " 'weight': 272,\n",
       " 'small/medium.▁': 273,\n",
       " 'graphic▁': 274,\n",
       " 'kit▁': 275,\n",
       " 'drawstr': 276,\n",
       " 'ō': 277,\n",
       " 'beige.▁': 278,\n",
       " 'stone.▁': 279,\n",
       " 'cigarette▁': 280,\n",
       " 'revival,▁': 281,\n",
       " 'sunglasses▁': 282,\n",
       " 'ign▁': 283,\n",
       " 'padded,▁': 284,\n",
       " 'xxxxxxxxxxx': 285,\n",
       " 'fantas': 286,\n",
       " '⁄': 287,\n",
       " 'distresse': 288,\n",
       " 'waffle▁': 289,\n",
       " '(both▁': 290,\n",
       " '!!!*▁': 291,\n",
       " 'vad': 292,\n",
       " 'team': 293,\n",
       " 'croco': 294,\n",
       " 'content▁': 295,\n",
       " '26)▁': 296,\n",
       " 'mom▁': 297,\n",
       " 'ortho': 298,\n",
       " 'quo': 299,\n",
       " '•great▁': 300,\n",
       " 'there!▁': 301,\n",
       " '8\"': 302,\n",
       " 'elastic': 303,\n",
       " 'stuff,▁': 304,\n",
       " 'hanna': 305,\n",
       " 'bohemi': 306,\n",
       " 'braided▁': 307,\n",
       " 'triple▁': 308,\n",
       " 'amelia▁': 309,\n",
       " 'fall!▁': 310,\n",
       " 'judge▁': 311,\n",
       " 'active,▁': 312,\n",
       " 'yum▁': 313,\n",
       " 'ward▁': 314,\n",
       " 'ak▁': 315,\n",
       " 'y!!!▁': 316,\n",
       " 'makeup': 317,\n",
       " 'splash▁': 318,\n",
       " 'zip,▁': 319,\n",
       " 'holidays,▁': 320,\n",
       " 'prices!▁': 321,\n",
       " 'products.▁': 322,\n",
       " 'blonde▁': 323,\n",
       " 'chrom': 324,\n",
       " 'tation▁': 325,\n",
       " 'covers.▁': 326,\n",
       " 'edges.▁': 327,\n",
       " 'rc▁': 328,\n",
       " 'deluxe,▁': 329,\n",
       " 'push-': 330,\n",
       " 'coming▁': 331,\n",
       " 'anced▁': 332,\n",
       " 'cutouts▁': 333,\n",
       " 'condition!!▁': 334,\n",
       " 'rhinestone▁': 335,\n",
       " 'slime!▁': 336,\n",
       " 'extr': 337,\n",
       " 'liq▁': 338,\n",
       " 'price.▁': 339,\n",
       " '32-': 340,\n",
       " 'ulta': 341,\n",
       " 'marks.▁': 342,\n",
       " 'end▁': 343,\n",
       " 'dressing▁': 344,\n",
       " 'cookies▁': 345,\n",
       " '▶': 346,\n",
       " '4$▁': 347,\n",
       " '(9': 348,\n",
       " 'rare': 349,\n",
       " '0-': 350,\n",
       " '\\u3000': 351,\n",
       " 'detergent.▁': 352,\n",
       " 'swatch': 353,\n",
       " 'koala▁': 354,\n",
       " 'gigab': 355,\n",
       " 'wi-fi▁': 356,\n",
       " 'breathable▁': 357,\n",
       " 'bra.▁': 358,\n",
       " 'goodies▁': 359,\n",
       " 'more▁': 360,\n",
       " 'f!▁': 361,\n",
       " 'appear': 362,\n",
       " 'generic▁': 363,\n",
       " 'tumblers▁': 364,\n",
       " 'vspink': 365,\n",
       " 'glow▁': 366,\n",
       " 'processed▁': 367,\n",
       " 'consist▁': 368,\n",
       " 'honest▁': 369,\n",
       " 'reason.▁': 370,\n",
       " 'box!▁': 371,\n",
       " 'drawstring.▁': 372,\n",
       " '18x': 373,\n",
       " 'florida▁': 374,\n",
       " 'weather▁': 375,\n",
       " 'decent▁': 376,\n",
       " '...................': 377,\n",
       " '1,': 378,\n",
       " 'sheep▁': 379,\n",
       " 'hone▁': 380,\n",
       " 'dense▁': 381,\n",
       " 'frozen▁': 382,\n",
       " 'loye': 383,\n",
       " '☑': 384,\n",
       " 'jewelry!▁': 385,\n",
       " 'm8▁': 386,\n",
       " 'drawers▁': 387,\n",
       " '[rm]-[rm]▁': 388,\n",
       " 'poppy▁': 389,\n",
       " 'location▁': 390,\n",
       " 'clo': 391,\n",
       " 'bes': 392,\n",
       " 'huf▁': 393,\n",
       " 'neutral▁': 394,\n",
       " 'yan': 395,\n",
       " 'ici': 396,\n",
       " 'serious▁': 397,\n",
       " 'dun': 398,\n",
       " 'mg▁': 399,\n",
       " 'el▁': 400,\n",
       " 'love,▁': 401,\n",
       " 'bel▁': 402,\n",
       " 'inclu': 403,\n",
       " 'first-class▁': 404,\n",
       " 'orial▁': 405,\n",
       " '6-12m▁': 406,\n",
       " 'simulation▁': 407,\n",
       " '-top▁': 408,\n",
       " 'happened▁': 409,\n",
       " 'colors/': 410,\n",
       " 'sion,▁': 411,\n",
       " 'pep-▁': 412,\n",
       " \"you've▁\": 413,\n",
       " 'capacity:▁': 414,\n",
       " 'erase▁': 415,\n",
       " '[rm]▁': 416,\n",
       " 'binder▁': 417,\n",
       " 'shopp': 418,\n",
       " 'gas': 419,\n",
       " 'th,▁': 420,\n",
       " 'strengthen': 421,\n",
       " '100-': 422,\n",
       " 'ing!!!!▁': 423,\n",
       " '5s,▁': 424,\n",
       " 'female▁': 425,\n",
       " '(fits▁': 426,\n",
       " 'allergic▁': 427,\n",
       " 'go,▁': 428,\n",
       " '2.5': 429,\n",
       " 'sofia▁': 430,\n",
       " 'flaws!▁': 431,\n",
       " 'statement▁': 432,\n",
       " 'necklace': 433,\n",
       " 'feels▁': 434,\n",
       " 'glue,▁': 435,\n",
       " 'tangerine▁': 436,\n",
       " 'vietnam': 437,\n",
       " 'somewhat▁': 438,\n",
       " 'sleek▁': 439,\n",
       " 'thru▁': 440,\n",
       " 'guess,▁': 441,\n",
       " 'hem,▁': 442,\n",
       " '!!!!!▁': 443,\n",
       " 'tum': 444,\n",
       " 'weightless▁': 445,\n",
       " 'canvas': 446,\n",
       " 'eras': 447,\n",
       " 'wood': 448,\n",
       " 'ther,▁': 449,\n",
       " 'youn': 450,\n",
       " 'loves▁': 451,\n",
       " 'tester▁': 452,\n",
       " 'switch.▁': 453,\n",
       " 'earrings▁': 454,\n",
       " 've,▁': 455,\n",
       " 'listings▁': 456,\n",
       " 'lands▁': 457,\n",
       " 'ahh': 458,\n",
       " 'arm,▁': 459,\n",
       " 'must': 460,\n",
       " 'notes▁': 461,\n",
       " \"5'\": 462,\n",
       " '-a▁': 463,\n",
       " '11.5': 464,\n",
       " 'more!▁': 465,\n",
       " 'sleeve!▁': 466,\n",
       " 'firms▁': 467,\n",
       " 'temporary▁': 468,\n",
       " 'original▁': 469,\n",
       " '.....': 470,\n",
       " 'borax,▁': 471,\n",
       " 'sh.▁': 472,\n",
       " 'crystals▁': 473,\n",
       " 'male▁': 474,\n",
       " 'damage,▁': 475,\n",
       " 'hair,▁': 476,\n",
       " 'paper.▁': 477,\n",
       " 'dr▁': 478,\n",
       " 'first-': 479,\n",
       " 'fit.▁': 480,\n",
       " 'guns▁': 481,\n",
       " 'made▁': 482,\n",
       " 'blu-ray▁': 483,\n",
       " 'thankyou▁': 484,\n",
       " 'lover.▁': 485,\n",
       " 'ari': 486,\n",
       " '\"l▁': 487,\n",
       " 'wispi': 488,\n",
       " 'denim.▁': 489,\n",
       " '***▁': 490,\n",
       " 'pixar▁': 491,\n",
       " 'ings!▁': 492,\n",
       " 'spirit': 493,\n",
       " 'lipli': 494,\n",
       " 'mickey▁': 495,\n",
       " 'drops▁': 496,\n",
       " 'nordstrom.▁': 497,\n",
       " 'fran': 498,\n",
       " 'put▁': 499,\n",
       " '(see▁': 500,\n",
       " 'fol': 501,\n",
       " 'nutrients▁': 502,\n",
       " 'tu': 503,\n",
       " 'caa▁': 504,\n",
       " 'instruction▁': 505,\n",
       " 'nav': 506,\n",
       " 'sweatshirts▁': 507,\n",
       " 'makeup.▁': 508,\n",
       " 'remove▁': 509,\n",
       " 'davidson▁': 510,\n",
       " 'rep': 511,\n",
       " 'per,▁': 512,\n",
       " 'bodice▁': 513,\n",
       " 'nordstrom': 514,\n",
       " 'clam': 515,\n",
       " 'tone▁': 516,\n",
       " 'ml': 517,\n",
       " 'reaction▁': 518,\n",
       " 'cor▁': 519,\n",
       " 'tom▁': 520,\n",
       " 'y,▁': 521,\n",
       " 'tening▁': 522,\n",
       " 'hawaiian,▁': 523,\n",
       " 'real.▁': 524,\n",
       " 'p▁': 525,\n",
       " 'rocket▁': 526,\n",
       " 'spider': 527,\n",
       " 'ch:▁': 528,\n",
       " 'trimming▁': 529,\n",
       " 'comic▁': 530,\n",
       " 'class,▁': 531,\n",
       " 'iphone': 532,\n",
       " 'heels,▁': 533,\n",
       " '(with▁': 534,\n",
       " 'product.▁': 535,\n",
       " 'around!▁': 536,\n",
       " '.comes▁': 537,\n",
       " 'lla▁': 538,\n",
       " 'toge': 539,\n",
       " 'cowboys▁': 540,\n",
       " 'ami▁': 541,\n",
       " 'ler▁': 542,\n",
       " 'plant▁': 543,\n",
       " 'on!▁': 544,\n",
       " 'bracelet▁': 545,\n",
       " '-mini▁': 546,\n",
       " 'semi▁': 547,\n",
       " 'handy▁': 548,\n",
       " 'amii': 549,\n",
       " '/[rm]▁': 550,\n",
       " 'gus': 551,\n",
       " 'str': 552,\n",
       " 'based▁': 553,\n",
       " 'needed.▁': 554,\n",
       " 'requires▁': 555,\n",
       " 'separately)▁': 556,\n",
       " 'love▁': 557,\n",
       " 'class.▁': 558,\n",
       " '️free▁': 559,\n",
       " 'bermuda▁': 560,\n",
       " 'boat▁': 561,\n",
       " 'win': 562,\n",
       " 'prettyinpink': 563,\n",
       " 'storage.▁': 564,\n",
       " 'try▁': 565,\n",
       " 'a-': 566,\n",
       " 'fourth▁': 567,\n",
       " 'piling▁': 568,\n",
       " '✨i▁': 569,\n",
       " 'rain': 570,\n",
       " 'gina▁': 571,\n",
       " 'queen▁': 572,\n",
       " 'love.▁': 573,\n",
       " 'move': 574,\n",
       " 'spring': 575,\n",
       " 'magnets▁': 576,\n",
       " 'so': 577,\n",
       " 'blood': 578,\n",
       " 'vers': 579,\n",
       " '---': 580,\n",
       " 'red▁': 581,\n",
       " 'book.▁': 582,\n",
       " 'shield▁': 583,\n",
       " 'ysl▁': 584,\n",
       " '–▁': 585,\n",
       " 'brand)▁': 586,\n",
       " 'aside▁': 587,\n",
       " 'me,▁': 588,\n",
       " 'factory▁': 589,\n",
       " 'sticker.▁': 590,\n",
       " 'head,▁': 591,\n",
       " 'topic': 592,\n",
       " 'mascara▁': 593,\n",
       " 'g2▁': 594,\n",
       " 'washed,▁': 595,\n",
       " 'defen': 596,\n",
       " 'quick▁': 597,\n",
       " 'lbs.▁': 598,\n",
       " '210▁': 599,\n",
       " 'ed': 600,\n",
       " 'ball': 601,\n",
       " 'ext': 602,\n",
       " 'save!!▁': 603,\n",
       " 'gh▁': 604,\n",
       " 'ne.▁': 605,\n",
       " '#pink▁': 606,\n",
       " 'authentic!!▁': 607,\n",
       " 'flir': 608,\n",
       " 'icloud▁': 609,\n",
       " 'steep▁': 610,\n",
       " 'clu': 611,\n",
       " 'snack▁': 612,\n",
       " '20\"▁': 613,\n",
       " 'tags.▁': 614,\n",
       " 'cosmetics,▁': 615,\n",
       " '--▁': 616,\n",
       " 'bench▁': 617,\n",
       " 'mask.▁': 618,\n",
       " 'chargers/': 619,\n",
       " '27▁': 620,\n",
       " 'tter▁': 621,\n",
       " '-5▁': 622,\n",
       " 'prayer▁': 623,\n",
       " 'teething▁': 624,\n",
       " 'reduce▁': 625,\n",
       " 'puffiness▁': 626,\n",
       " 'mark': 627,\n",
       " 'issue,▁': 628,\n",
       " 'exceed▁': 629,\n",
       " '300': 630,\n",
       " 'assort': 631,\n",
       " 'butterfly,▁': 632,\n",
       " 'oney▁': 633,\n",
       " 'is': 634,\n",
       " 'activity▁': 635,\n",
       " 'wic': 636,\n",
       " 'tracked▁': 637,\n",
       " 'nies▁': 638,\n",
       " 'personalized▁': 639,\n",
       " 'vinyl.▁': 640,\n",
       " 'slip▁': 641,\n",
       " '13.5\"▁': 642,\n",
       " '•price▁': 643,\n",
       " 'shop!▁': 644,\n",
       " '0.33▁': 645,\n",
       " 'descriptions▁': 646,\n",
       " 'facili': 647,\n",
       " '♥️': 648,\n",
       " 'marg': 649,\n",
       " 'hog▁': 650,\n",
       " 'mother': 651,\n",
       " 'ble': 652,\n",
       " 'barcode▁': 653,\n",
       " 'network▁': 654,\n",
       " 'sculp': 655,\n",
       " 'unable▁': 656,\n",
       " '(choose▁': 657,\n",
       " 'touching▁': 658,\n",
       " 'adapter': 659,\n",
       " 'dots▁': 660,\n",
       " 'dow': 661,\n",
       " 'aurora▁': 662,\n",
       " 'sending▁': 663,\n",
       " 'skor': 664,\n",
       " 'identi': 665,\n",
       " 'heavi': 666,\n",
       " 'attached.▁': 667,\n",
       " 'eviden': 668,\n",
       " 'anno': 669,\n",
       " 'with.▁': 670,\n",
       " 'color:': 671,\n",
       " 'rosette▁': 672,\n",
       " \"s'▁\": 673,\n",
       " 'occur▁': 674,\n",
       " 'l,▁': 675,\n",
       " 'sliding▁': 676,\n",
       " 'oily▁': 677,\n",
       " 'daughter.▁': 678,\n",
       " 'reddish▁': 679,\n",
       " 'such▁': 680,\n",
       " 'direction▁': 681,\n",
       " 'ses,▁': 682,\n",
       " 'seri': 683,\n",
       " 'sue': 684,\n",
       " 'gym▁': 685,\n",
       " 'brown,▁': 686,\n",
       " 'bliss▁': 687,\n",
       " 'slin': 688,\n",
       " 'naruto▁': 689,\n",
       " '(worn▁': 690,\n",
       " '15\"▁': 691,\n",
       " 'edition~▁': 692,\n",
       " 'calming▁': 693,\n",
       " 'grape▁': 694,\n",
       " 'ordin': 695,\n",
       " 'extend': 696,\n",
       " 'nou': 697,\n",
       " 'report▁': 698,\n",
       " 'people,▁': 699,\n",
       " 'duplicate▁': 700,\n",
       " 'sweat-': 701,\n",
       " 'agen▁': 702,\n",
       " 'cellulite▁': 703,\n",
       " 'works,▁': 704,\n",
       " 'grillz▁': 705,\n",
       " '00$▁': 706,\n",
       " 'happi': 707,\n",
       " 'plus.▁': 708,\n",
       " 'marshall': 709,\n",
       " 'port▁': 710,\n",
       " 'radical▁': 711,\n",
       " 'dv': 712,\n",
       " 'g)▁': 713,\n",
       " 'sharp▁': 714,\n",
       " 'home:)▁': 715,\n",
       " 'beginning▁': 716,\n",
       " 'lay': 717,\n",
       " 'gril': 718,\n",
       " 'detail!▁': 719,\n",
       " 'double▁': 720,\n",
       " 'anymore▁': 721,\n",
       " '▶️': 722,\n",
       " 'unexpected▁': 723,\n",
       " '/or▁': 724,\n",
       " 'disc,▁': 725,\n",
       " 'y.▁': 726,\n",
       " \"haven't▁\": 727,\n",
       " 'flex': 728,\n",
       " 'dc▁': 729,\n",
       " \"a's▁\": 730,\n",
       " 't?▁': 731,\n",
       " 'son,▁': 732,\n",
       " 'lips,▁': 733,\n",
       " 'ones': 734,\n",
       " 'fuschia▁': 735,\n",
       " 'alterna': 736,\n",
       " 'hag': 737,\n",
       " 'cracked▁': 738,\n",
       " '$$$': 739,\n",
       " 'page.▁': 740,\n",
       " 'bumper▁': 741,\n",
       " 'crew▁': 742,\n",
       " 'works:▁': 743,\n",
       " 'lang': 744,\n",
       " 'occasion.▁': 745,\n",
       " 'boutique,▁': 746,\n",
       " 'you’': 747,\n",
       " 'buyers.▁': 748,\n",
       " 'thirty▁': 749,\n",
       " 'thir': 750,\n",
       " 'new]▁': 751,\n",
       " 'word': 752,\n",
       " '✔brand▁': 753,\n",
       " '3,▁': 754,\n",
       " 'scar': 755,\n",
       " 'accur': 756,\n",
       " '45%▁': 757,\n",
       " 'assure▁': 758,\n",
       " 'station▁': 759,\n",
       " 'looked▁': 760,\n",
       " 'not▁': 761,\n",
       " 'disc.▁': 762,\n",
       " 'ect▁': 763,\n",
       " 'reflected▁': 764,\n",
       " 'hesitate▁': 765,\n",
       " '9,': 766,\n",
       " '74': 767,\n",
       " 'ding,▁': 768,\n",
       " 'nautic': 769,\n",
       " 'ne▁': 770,\n",
       " 'hook▁': 771,\n",
       " 'hologr': 772,\n",
       " 'display': 773,\n",
       " 'ative▁': 774,\n",
       " 'missguided▁': 775,\n",
       " 'damaging▁': 776,\n",
       " 'extra': 777,\n",
       " 'bead▁': 778,\n",
       " 'al/': 779,\n",
       " 'pocket.▁': 780,\n",
       " 'm-▁': 781,\n",
       " 'alo▁': 782,\n",
       " 'ses!▁': 783,\n",
       " 'thousands▁': 784,\n",
       " 'train▁': 785,\n",
       " 'ink▁': 786,\n",
       " 'coco': 787,\n",
       " 'ios▁': 788,\n",
       " 'earphones,▁': 789,\n",
       " 'through.▁': 790,\n",
       " '.i▁': 791,\n",
       " 'cap.▁': 792,\n",
       " 'enhanc': 793,\n",
       " '9\"▁': 794,\n",
       " 'cassie▁': 795,\n",
       " 'wild': 796,\n",
       " 'comes▁': 797,\n",
       " 'comp': 798,\n",
       " 'front': 799,\n",
       " 'ges': 800,\n",
       " '---------------------------': 801,\n",
       " '12%▁': 802,\n",
       " 'fan': 803,\n",
       " 'samples▁': 804,\n",
       " 'xl/': 805,\n",
       " 'dryness.▁': 806,\n",
       " 'places.▁': 807,\n",
       " 'buyer▁': 808,\n",
       " 'wars▁': 809,\n",
       " 'tb': 810,\n",
       " 'dang': 811,\n",
       " 'seduction▁': 812,\n",
       " 'sanrio▁': 813,\n",
       " 'ing-': 814,\n",
       " '40': 815,\n",
       " 'kitchen,▁': 816,\n",
       " '-----------------': 817,\n",
       " 'subst': 818,\n",
       " 'tele': 819,\n",
       " 'negotiable▁': 820,\n",
       " 'medium)▁': 821,\n",
       " 'alloy▁': 822,\n",
       " 'instructions:▁': 823,\n",
       " 'thin.▁': 824,\n",
       " 'ᴠ': 825,\n",
       " 'pictures,▁': 826,\n",
       " 'required▁': 827,\n",
       " 'black,▁': 828,\n",
       " '●●●●': 829,\n",
       " 'mounted▁': 830,\n",
       " 'living▁': 831,\n",
       " 'oversized▁': 832,\n",
       " 'cleaning.▁': 833,\n",
       " 'shades,▁': 834,\n",
       " 'hot.▁': 835,\n",
       " 'sleep': 836,\n",
       " 'impuri': 837,\n",
       " 'bottles.▁': 838,\n",
       " '+—•▁': 839,\n",
       " 'inch.▁': 840,\n",
       " 'h19': 841,\n",
       " 'rule▁': 842,\n",
       " 'ced▁': 843,\n",
       " 'review!▁': 844,\n",
       " 'poké': 845,\n",
       " 'blocke': 846,\n",
       " 'ofra▁': 847,\n",
       " 'marker▁': 848,\n",
       " 'gloss,▁': 849,\n",
       " 'a.': 850,\n",
       " 'sz▁': 851,\n",
       " '‼️price▁': 852,\n",
       " 'sell.▁': 853,\n",
       " 'cool.▁': 854,\n",
       " '76': 855,\n",
       " 'mani▁': 856,\n",
       " 'grip.▁': 857,\n",
       " 'n': 858,\n",
       " 'fading,▁': 859,\n",
       " 'jean': 860,\n",
       " 'mysel': 861,\n",
       " 'jacket,▁': 862,\n",
       " 'handle▁': 863,\n",
       " 'shadow,▁': 864,\n",
       " 'way.▁': 865,\n",
       " 'released▁': 866,\n",
       " 'clothes▁': 867,\n",
       " 'size?▁': 868,\n",
       " 'pink,▁': 869,\n",
       " 'luxie▁': 870,\n",
       " 'blending▁': 871,\n",
       " 'compatible.▁': 872,\n",
       " '4.5': 873,\n",
       " '-ship▁': 874,\n",
       " 'vie': 875,\n",
       " 'vapor▁': 876,\n",
       " 'stars▁': 877,\n",
       " '2.5oz▁': 878,\n",
       " 'cars,▁': 879,\n",
       " 'simple,▁': 880,\n",
       " 'series.▁': 881,\n",
       " 'referen': 882,\n",
       " 'yee': 883,\n",
       " 'ies▁': 884,\n",
       " 'let.▁': 885,\n",
       " '~~~': 886,\n",
       " 'conven': 887,\n",
       " 'letto▁': 888,\n",
       " '《《《《《': 889,\n",
       " 'bracelet,▁': 890,\n",
       " 'ʀᴇ': 891,\n",
       " 'novel▁': 892,\n",
       " 'buyers▁': 893,\n",
       " 'ant,▁': 894,\n",
       " 'nation▁': 895,\n",
       " 's..▁': 896,\n",
       " 'symbo': 897,\n",
       " 'straigh': 898,\n",
       " 'light-weight▁': 899,\n",
       " 'tta▁': 900,\n",
       " 'people▁': 901,\n",
       " 'tongu': 902,\n",
       " '64▁': 903,\n",
       " 'oyster▁': 904,\n",
       " 'yoga▁': 905,\n",
       " 'gives▁': 906,\n",
       " 'reli': 907,\n",
       " 'rated▁': 908,\n",
       " 'lor': 909,\n",
       " 'name': 910,\n",
       " 'gin▁': 911,\n",
       " 'toy.▁': 912,\n",
       " 'perio': 913,\n",
       " 'bundl': 914,\n",
       " 'ween▁': 915,\n",
       " 'rof': 916,\n",
       " 'replacement▁': 917,\n",
       " 'sies▁': 918,\n",
       " 'fix.▁': 919,\n",
       " 'sling▁': 920,\n",
       " 'inserts▁': 921,\n",
       " 'cele': 922,\n",
       " 'nigh': 923,\n",
       " 'sexy!▁': 924,\n",
       " 'super': 925,\n",
       " 'irrit': 926,\n",
       " 'resin▁': 927,\n",
       " 'lumin': 928,\n",
       " 'burlap▁': 929,\n",
       " 'easily.▁': 930,\n",
       " 'absorbing▁': 931,\n",
       " 'coverage.▁': 932,\n",
       " 'brav': 933,\n",
       " 'mun': 934,\n",
       " 'corner▁': 935,\n",
       " 'insurance▁': 936,\n",
       " 'most.▁': 937,\n",
       " 'ó': 938,\n",
       " 'test▁': 939,\n",
       " 'photo,▁': 940,\n",
       " 'days,▁': 941,\n",
       " 'norm': 942,\n",
       " 'primer,▁': 943,\n",
       " '3-6m▁': 944,\n",
       " 'soaps▁': 945,\n",
       " 'safe': 946,\n",
       " 'ane,▁': 947,\n",
       " 'wii▁': 948,\n",
       " 'firm!!▁': 949,\n",
       " 'speak': 950,\n",
       " 'floss▁': 951,\n",
       " '•': 952,\n",
       " 'whole▁': 953,\n",
       " 'gras': 954,\n",
       " 'stick▁': 955,\n",
       " '10k▁': 956,\n",
       " 'fring': 957,\n",
       " 'search:▁': 958,\n",
       " 'reset▁': 959,\n",
       " 'enhancer▁': 960,\n",
       " 'bras▁': 961,\n",
       " 'ins▁': 962,\n",
       " 'books.▁': 963,\n",
       " 'match▁': 964,\n",
       " 'stric': 965,\n",
       " 'closest▁': 966,\n",
       " '◇for▁': 967,\n",
       " '✖️✖️✖️✖️✖️✖️✖️✖️✖️✖️✖️✖️✖️✖️✖️': 968,\n",
       " 'porcela': 969,\n",
       " 'everyone!▁': 970,\n",
       " 'dail': 971,\n",
       " ',g2▁': 972,\n",
       " 'scalp▁': 973,\n",
       " 'today.▁': 974,\n",
       " 'tered▁': 975,\n",
       " 'ing.': 976,\n",
       " 'presents▁': 977,\n",
       " 'bts▁': 978,\n",
       " 'rack': 979,\n",
       " '(7': 980,\n",
       " 'tarte▁': 981,\n",
       " 'nice▁': 982,\n",
       " 'lions▁': 983,\n",
       " '+': 984,\n",
       " 'cep': 985,\n",
       " 'inve': 986,\n",
       " 'fer': 987,\n",
       " 'shat': 988,\n",
       " 'pierce▁': 989,\n",
       " 'used/': 990,\n",
       " 'grateful▁': 991,\n",
       " 'ty': 992,\n",
       " 'dew': 993,\n",
       " 'fru': 994,\n",
       " 'pear▁': 995,\n",
       " 'seal': 996,\n",
       " 'kathy▁': 997,\n",
       " 'umbre': 998,\n",
       " 'shirts.▁': 999,\n",
       " 'thanks': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe._vocab_stats['92%▁']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['byte-pair-encoder.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(bpe, 'byte-pair-encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode/decode some strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4416571"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.67 s, sys: 69.8 ms, total: 4.74 s\n",
      "Wall time: 4.74 s\n"
     ]
    }
   ],
   "source": [
    "%time tokens = bpe.transform(bpe_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 282 ms, sys: 6.55 ms, total: 289 ms\n",
      "Wall time: 289 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hollister▁',\n",
       " 'jean▁',\n",
       " 'shorts,▁',\n",
       " 'size▁',\n",
       " '9.▁',\n",
       " 'great▁',\n",
       " 'condition.▁',\n",
       " 'prices▁',\n",
       " 'are▁',\n",
       " 'not▁',\n",
       " 'firm▁',\n",
       " 'check▁',\n",
       " 'my▁',\n",
       " 'closet▁',\n",
       " 'and▁',\n",
       " 'save▁',\n",
       " 'on▁',\n",
       " 'bundles▁',\n",
       " 'f21▁',\n",
       " '(m)▁',\n",
       " 'thigh▁',\n",
       " 'length▁',\n",
       " 'knitted▁',\n",
       " 'new▁',\n",
       " 'with▁',\n",
       " 'tag▁',\n",
       " 'no▁',\n",
       " 'bound',\n",
       " '<unk>',\n",
       " 's▁',\n",
       " 'christmas▁',\n",
       " 'leggings▁',\n",
       " 'santa▁',\n",
       " 'size▁',\n",
       " 'medium▁',\n",
       " '7-',\n",
       " '9▁',\n",
       " 'super▁',\n",
       " 'cute▁',\n",
       " 'but▁',\n",
       " 'not▁',\n",
       " 'love▁',\n",
       " 'on▁',\n",
       " 'me▁',\n",
       " ':(▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " 'lularoe▁',\n",
       " 'tc▁',\n",
       " 'bear▁',\n",
       " 'leggings▁',\n",
       " 'with▁',\n",
       " 'a▁',\n",
       " 'black▁',\n",
       " 'background.▁',\n",
       " 'please▁',\n",
       " 'note▁',\n",
       " 'that▁',\n",
       " 'i▁',\n",
       " 'do▁',\n",
       " 'not▁',\n",
       " 'have▁',\n",
       " 'original▁',\n",
       " 'packaging▁',\n",
       " 'but▁',\n",
       " 'these▁',\n",
       " 'have▁',\n",
       " 'never▁',\n",
       " 'been▁',\n",
       " 'worn▁',\n",
       " 'nor▁',\n",
       " 'washed.▁',\n",
       " 'check▁',\n",
       " 'out▁',\n",
       " 'my▁',\n",
       " 'other▁',\n",
       " 'listings!▁',\n",
       " 'in▁',\n",
       " 'great▁',\n",
       " 'condition!▁',\n",
       " 'worn▁',\n",
       " 'only▁',\n",
       " 'a▁',\n",
       " 'few▁',\n",
       " 'times▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " 'one▁',\n",
       " 'taupe',\n",
       " ',▁',\n",
       " 'one▁',\n",
       " 'dark▁',\n",
       " 'olive.▁',\n",
       " 'adjustable▁',\n",
       " 'neck▁',\n",
       " 'and▁',\n",
       " 'back▁',\n",
       " '<unk>',\n",
       " '.▁',\n",
       " 'from▁',\n",
       " '<unk>',\n",
       " ':▁',\n",
       " 'lily▁',\n",
       " 'rain▁',\n",
       " 'both▁',\n",
       " 'size▁',\n",
       " 'small.▁',\n",
       " 'no▁',\n",
       " 'description▁',\n",
       " 'yet▁',\n",
       " 'matching▁',\n",
       " 'mommy▁',\n",
       " 'and▁',\n",
       " 'me▁',\n",
       " 'shirt▁',\n",
       " 'set.▁',\n",
       " 'please▁',\n",
       " 'message▁',\n",
       " 'me▁',\n",
       " 'sizes.▁',\n",
       " 'i▁',\n",
       " 'also▁',\n",
       " 'offer▁',\n",
       " 'custom▁',\n",
       " 'shirts▁',\n",
       " 'at▁',\n",
       " '[rm]▁',\n",
       " 'each.▁',\n",
       " 'no▁',\n",
       " 'description▁',\n",
       " 'yet▁',\n",
       " 'kids▁',\n",
       " 'size▁',\n",
       " '2▁',\n",
       " 'first▁',\n",
       " 'dress▁',\n",
       " 'is▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'second▁',\n",
       " 'dress▁',\n",
       " 'is▁',\n",
       " 'leggings▁',\n",
       " 'material.▁',\n",
       " 'good▁',\n",
       " 'condition▁',\n",
       " 'ralph▁',\n",
       " 'lauren▁',\n",
       " 'fragrance▁',\n",
       " 'hat▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " 'never▁',\n",
       " 'worn.▁',\n",
       " 'once▁',\n",
       " 'size,▁',\n",
       " 'no▁',\n",
       " 'tags.▁',\n",
       " 'not▁',\n",
       " 'sure▁',\n",
       " 'of▁',\n",
       " 'brand▁',\n",
       " 'free▁',\n",
       " 'shipping!▁',\n",
       " 'package▁',\n",
       " 'includes▁',\n",
       " '1▁',\n",
       " 'sponge▁',\n",
       " 'for▁',\n",
       " '[rm]▁',\n",
       " 'plus▁',\n",
       " 'free▁',\n",
       " 'blackhead▁',\n",
       " 'removal▁',\n",
       " 'mud▁',\n",
       " 'mask',\n",
       " '!▁',\n",
       " 'please▁',\n",
       " 'adv',\n",
       " 'ise▁',\n",
       " 'what▁',\n",
       " 'color▁',\n",
       " 'you▁',\n",
       " 'want▁',\n",
       " 'in▁',\n",
       " 'your▁',\n",
       " 'order▁',\n",
       " 'details.▁',\n",
       " 'if▁',\n",
       " 'not,▁',\n",
       " 'i▁',\n",
       " 'will▁',\n",
       " 'send▁',\n",
       " 'random',\n",
       " '.▁',\n",
       " 'the▁',\n",
       " '<unk>',\n",
       " 'pon',\n",
       " 'ge!▁',\n",
       " '•the▁',\n",
       " 'silicone▁',\n",
       " 'sponge▁',\n",
       " 'like▁',\n",
       " 'makeup▁',\n",
       " 'applicator▁',\n",
       " 'was▁',\n",
       " 'designed▁',\n",
       " 'to▁',\n",
       " 'completely▁',\n",
       " 'eliminate▁',\n",
       " '<unk>',\n",
       " 'd▁',\n",
       " 'makeup.▁',\n",
       " 'while▁',\n",
       " 'sponge▁',\n",
       " 'applicators▁',\n",
       " 'soak▁',\n",
       " \"i'm▁\",\n",
       " 'large▁',\n",
       " 'amounts▁',\n",
       " 'of▁',\n",
       " 'makeup▁',\n",
       " 'during▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'the▁',\n",
       " '\"',\n",
       " '<unk>',\n",
       " 'pon',\n",
       " 'ge',\n",
       " '\"▁',\n",
       " 'perfectly▁',\n",
       " 'applies▁',\n",
       " '&▁',\n",
       " 'blends▁',\n",
       " 'your▁',\n",
       " 'foundation,▁',\n",
       " 'blush,▁',\n",
       " 'highlighter,▁',\n",
       " '&▁',\n",
       " 'concealer▁',\n",
       " 'w/out▁',\n",
       " 'absorbing▁',\n",
       " 'one▁',\n",
       " 'drop',\n",
       " '!▁',\n",
       " 'only▁',\n",
       " 'half▁',\n",
       " 'the▁',\n",
       " 'amount▁',\n",
       " 'of▁',\n",
       " 'product▁',\n",
       " 'is▁',\n",
       " 'needed▁',\n",
       " 'as▁',\n",
       " 'compared▁',\n",
       " 'to▁',\n",
       " 'a▁',\n",
       " 'traditional▁',\n",
       " 'sponge▁',\n",
       " 'or▁',\n",
       " 'brush▁',\n",
       " 'applicators.▁',\n",
       " 'size▁',\n",
       " 'small▁',\n",
       " 'very▁',\n",
       " 'comfy▁',\n",
       " 'little▁',\n",
       " 'snug▁',\n",
       " 'on▁',\n",
       " 'me▁',\n",
       " 'so▁',\n",
       " \"i'm▁\",\n",
       " 'selling▁',\n",
       " 'it▁',\n",
       " 'good▁',\n",
       " 'condition.▁',\n",
       " 'free▁',\n",
       " 'shipping!▁',\n",
       " 'great▁',\n",
       " 'price▁',\n",
       " 'and▁',\n",
       " 'they▁',\n",
       " 'take▁',\n",
       " '10%',\n",
       " '.▁',\n",
       " 'bought▁',\n",
       " 'from▁',\n",
       " 'dick',\n",
       " 's▁',\n",
       " 'sporting▁',\n",
       " 'good',\n",
       " 's.▁',\n",
       " 'ww',\n",
       " 'p▁',\n",
       " 'size:▁',\n",
       " '0▁',\n",
       " 'ae▁',\n",
       " 'super▁',\n",
       " 'stretch▁',\n",
       " 'very▁',\n",
       " 'comfy▁',\n",
       " 'just▁',\n",
       " 'too▁',\n",
       " 'small▁',\n",
       " 'for▁',\n",
       " 'me▁',\n",
       " 'now▁',\n",
       " 'set▁',\n",
       " 'if▁',\n",
       " '2▁',\n",
       " '0.5▁',\n",
       " 'oz▁',\n",
       " 'each▁',\n",
       " 'perfect▁',\n",
       " 'for▁',\n",
       " 'festivals▁',\n",
       " 'and▁',\n",
       " '<unk>',\n",
       " 's!▁',\n",
       " 'this▁',\n",
       " 'small,▁',\n",
       " 'canvas▁',\n",
       " 'black▁',\n",
       " 'backpack▁',\n",
       " 'has▁',\n",
       " 'two▁',\n",
       " '<unk>',\n",
       " '/',\n",
       " '<unk>',\n",
       " '.▁',\n",
       " 'the▁',\n",
       " 'straps▁',\n",
       " 'on▁',\n",
       " 'the▁',\n",
       " 'back▁',\n",
       " 'can▁',\n",
       " 'zip▁',\n",
       " 'up▁',\n",
       " 'into▁',\n",
       " 'one▁',\n",
       " 'strap.▁',\n",
       " 'fits▁',\n",
       " 'a▁',\n",
       " 'decent▁',\n",
       " 'amount▁',\n",
       " 'of▁',\n",
       " 'necessi',\n",
       " 'ties:▁',\n",
       " 'makeup,▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'wallet,▁',\n",
       " 'phone,▁',\n",
       " 'book,▁',\n",
       " 'etc.▁',\n",
       " 'has▁',\n",
       " 'some▁',\n",
       " 'paint▁',\n",
       " 'markings▁',\n",
       " 'on▁',\n",
       " 'the▁',\n",
       " 'handle.▁',\n",
       " 'pins▁',\n",
       " 'and▁',\n",
       " 'keychain▁',\n",
       " 'not▁',\n",
       " 'included.▁',\n",
       " 'sorry,▁',\n",
       " 'no▁',\n",
       " 'free▁',\n",
       " 'shipping▁',\n",
       " 'on▁',\n",
       " 'this▁',\n",
       " 'item!▁',\n",
       " 'free▁',\n",
       " 'shipping▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " '(no▁',\n",
       " 'box)▁',\n",
       " 'travel▁',\n",
       " 'size▁',\n",
       " '0.8',\n",
       " '▁',\n",
       " 'fl▁',\n",
       " 'oz▁',\n",
       " 'see▁',\n",
       " 'pics▁',\n",
       " 'for▁',\n",
       " 'size▁',\n",
       " 'comparison▁',\n",
       " 'this▁',\n",
       " 'cream▁',\n",
       " 'smells▁',\n",
       " '<unk>',\n",
       " 'oo▁',\n",
       " 'good.▁',\n",
       " 'i▁',\n",
       " 'use▁',\n",
       " 'on▁',\n",
       " 'my▁',\n",
       " 'entire▁',\n",
       " 'body▁',\n",
       " 'and▁',\n",
       " 'its▁',\n",
       " 'hydrating▁',\n",
       " 'and▁',\n",
       " 'the▁',\n",
       " 'smell▁',\n",
       " 'lasts▁',\n",
       " 'all▁',\n",
       " 'day.▁',\n",
       " 'love▁',\n",
       " 'it.▁',\n",
       " 'price▁',\n",
       " 'is▁',\n",
       " 'firm▁',\n",
       " '100%▁',\n",
       " 'authentic▁',\n",
       " ',▁',\n",
       " 'a▁',\n",
       " 'few▁',\n",
       " 'minor▁',\n",
       " 'flaws▁',\n",
       " 'as▁',\n",
       " 'pictured.▁',\n",
       " 'still▁',\n",
       " 'in▁',\n",
       " 'good▁',\n",
       " 'condition.▁',\n",
       " 'reasonable▁',\n",
       " 'offers▁',\n",
       " 'accepted.▁',\n",
       " 'new▁',\n",
       " 'bluray▁',\n",
       " 'and▁',\n",
       " 'dc▁',\n",
       " 'only▁',\n",
       " 'comes▁',\n",
       " 'in▁',\n",
       " 'cd▁',\n",
       " 'case▁',\n",
       " 'this▁',\n",
       " 'is▁',\n",
       " 'the▁',\n",
       " 'new▁',\n",
       " 'live▁',\n",
       " 'action▁',\n",
       " 'version▁',\n",
       " 'aluminum▁',\n",
       " 'fidget▁',\n",
       " '<unk>',\n",
       " ':▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " '.▁',\n",
       " 'comes▁',\n",
       " 'in▁',\n",
       " 'box.▁',\n",
       " 'use▁',\n",
       " 'jn▁',\n",
       " 'class,▁',\n",
       " 'work,▁',\n",
       " 'office,▁',\n",
       " 'travel,▁',\n",
       " 'etc.▁',\n",
       " 'reduce▁',\n",
       " 'stress▁',\n",
       " 'help▁',\n",
       " 'in▁',\n",
       " 'thinking▁',\n",
       " 'and▁',\n",
       " 'on▁',\n",
       " '<unk>',\n",
       " '.made▁',\n",
       " 'by▁',\n",
       " '<unk>',\n",
       " '.▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'never▁',\n",
       " 'worn▁',\n",
       " 'never▁',\n",
       " 'washed.▁',\n",
       " 'size▁',\n",
       " 'medium.▁',\n",
       " 'in▁',\n",
       " 'great▁',\n",
       " 'condition.▁',\n",
       " 'could▁',\n",
       " 'be▁',\n",
       " 'carried▁',\n",
       " 'as▁',\n",
       " 'wristlet.▁',\n",
       " 'very▁',\n",
       " 'versatile▁',\n",
       " 'size▁',\n",
       " '9▁',\n",
       " 'second▁',\n",
       " 'pair▁',\n",
       " 'of▁',\n",
       " 'flats▁',\n",
       " 'beige▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " 'in▁',\n",
       " 'factory▁',\n",
       " 'sealed▁',\n",
       " 'packaging▁',\n",
       " '-please▁',\n",
       " 'allow▁',\n",
       " '1-2▁',\n",
       " 'days▁',\n",
       " 'for▁',\n",
       " 'tracking▁',\n",
       " 'to▁',\n",
       " 'update▁',\n",
       " '-price▁',\n",
       " 'is▁',\n",
       " 'set▁',\n",
       " 'at▁',\n",
       " '5▁',\n",
       " '-please▁',\n",
       " 'remember▁',\n",
       " 'to▁',\n",
       " 'rate▁',\n",
       " 'after▁',\n",
       " 'you▁',\n",
       " 'purchase▁',\n",
       " 'and▁',\n",
       " 'recieve▁',\n",
       " '☆',\n",
       " 'this▁',\n",
       " 'ring▁',\n",
       " 'is▁',\n",
       " 'handmade.▁',\n",
       " '☆',\n",
       " 'available▁',\n",
       " 'in▁',\n",
       " 'any▁',\n",
       " 'size.▁',\n",
       " 'it▁',\n",
       " 'is▁',\n",
       " 'made▁',\n",
       " 'with▁',\n",
       " 'tarnish▁',\n",
       " 'resistant▁',\n",
       " 'wire▁',\n",
       " '&▁',\n",
       " 'an▁',\n",
       " '8mm▁',\n",
       " 'bead.▁',\n",
       " 'to▁',\n",
       " 'buy▁',\n",
       " 'a▁',\n",
       " 'ring',\n",
       " ':▁',\n",
       " 'buy▁',\n",
       " 'the▁',\n",
       " 'listing▁',\n",
       " 'first▁',\n",
       " 'and▁',\n",
       " 'then▁',\n",
       " 'message▁',\n",
       " 'me▁',\n",
       " 'the▁',\n",
       " 'size▁',\n",
       " 'you▁',\n",
       " 'want.▁',\n",
       " 'if▁',\n",
       " 'you▁',\n",
       " \"don't▁\",\n",
       " 'tell▁',\n",
       " 'me▁',\n",
       " 'the▁',\n",
       " 'size▁',\n",
       " 'i▁',\n",
       " 'will▁',\n",
       " 'have▁',\n",
       " 'to▁',\n",
       " 'cancel▁',\n",
       " 'your▁',\n",
       " 'order.▁',\n",
       " '1▁',\n",
       " 'ring',\n",
       " '-[rm]▁',\n",
       " '2▁',\n",
       " 'rings▁',\n",
       " 'or▁',\n",
       " 'more▁',\n",
       " '<unk>',\n",
       " 've▁',\n",
       " '[rm]▁',\n",
       " 'per▁',\n",
       " 'ring▁',\n",
       " 'there▁',\n",
       " 'may▁',\n",
       " 'be▁',\n",
       " 'minor▁',\n",
       " 'flaws/',\n",
       " '<unk>',\n",
       " 's▁',\n",
       " 'on▁',\n",
       " 'each▁',\n",
       " 'ring▁',\n",
       " 'because▁',\n",
       " 'they▁',\n",
       " 'are▁',\n",
       " 'handmade.▁',\n",
       " 'thanks▁',\n",
       " 'for▁',\n",
       " 'stopping▁',\n",
       " 'by!▁',\n",
       " '**************************************▁',\n",
       " 'drawing▁',\n",
       " 'stone▁',\n",
       " 'is▁',\n",
       " 'one▁',\n",
       " 'of▁',\n",
       " 'the▁',\n",
       " 'popular▁',\n",
       " 'names▁',\n",
       " 'of▁',\n",
       " '<unk>',\n",
       " 'ster,▁',\n",
       " 'which▁',\n",
       " 'may▁',\n",
       " '<unk>',\n",
       " 'ze▁',\n",
       " 'things▁',\n",
       " 'towards▁',\n",
       " 'you.▁',\n",
       " 'it▁',\n",
       " 'only▁',\n",
       " 'depends▁',\n",
       " 'on▁',\n",
       " 'what▁',\n",
       " 'you▁',\n",
       " 'need▁',\n",
       " 'in▁',\n",
       " 'your▁',\n",
       " 'life▁',\n",
       " 'physical▁',\n",
       " 'healing▁',\n",
       " '<unk>',\n",
       " ':▁',\n",
       " '<unk>',\n",
       " 'ster▁',\n",
       " 'is▁',\n",
       " 'a▁',\n",
       " 'well-',\n",
       " 'known▁',\n",
       " 'stone▁',\n",
       " 'which▁',\n",
       " 'may▁',\n",
       " 'heal▁',\n",
       " 'its▁',\n",
       " 'wear',\n",
       " 'er▁',\n",
       " 'against▁',\n",
       " 'tension,▁',\n",
       " 'head',\n",
       " 'ache',\n",
       " ',▁',\n",
       " 'lac',\n",
       " 'k▁',\n",
       " 'of▁',\n",
       " '<unk>',\n",
       " 'ion▁',\n",
       " '&▁',\n",
       " 'problems▁',\n",
       " 'with▁',\n",
       " 'pain',\n",
       " 'ful▁',\n",
       " 'join',\n",
       " 'ts.▁',\n",
       " 'emo',\n",
       " 'tional▁',\n",
       " 'healing▁',\n",
       " '<unk>',\n",
       " ':▁',\n",
       " '<unk>',\n",
       " 'ster▁',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " 's▁',\n",
       " '<unk>',\n",
       " '.▁',\n",
       " \"it's▁\",\n",
       " 'used▁',\n",
       " 'to▁',\n",
       " '<unk>',\n",
       " '▁',\n",
       " '<unk>',\n",
       " 's▁',\n",
       " 'of▁',\n",
       " 'peace▁',\n",
       " '&▁',\n",
       " '<unk>',\n",
       " 'ness.▁',\n",
       " 'the▁',\n",
       " 'powerful▁',\n",
       " 'energi',\n",
       " 'es▁',\n",
       " 'of▁',\n",
       " 'these▁',\n",
       " 'stones▁',\n",
       " 'may▁',\n",
       " 'be▁',\n",
       " 'used▁',\n",
       " 'to▁',\n",
       " 'increase▁',\n",
       " '<unk>',\n",
       " 'ili',\n",
       " 'ty▁',\n",
       " '&▁',\n",
       " 'let▁',\n",
       " 'the▁',\n",
       " 'wear',\n",
       " 'er▁',\n",
       " 'be▁',\n",
       " 'more▁',\n",
       " '<unk>',\n",
       " 'able.▁',\n",
       " 'info.▁',\n",
       " 'credit▁',\n",
       " 'to▁',\n",
       " '\"j',\n",
       " 'e',\n",
       " 'wel',\n",
       " '<unk>',\n",
       " '\"▁',\n",
       " '20▁',\n",
       " 'stamps▁',\n",
       " 'a▁',\n",
       " 'black▁',\n",
       " 'sweatshirt▁',\n",
       " 'white▁',\n",
       " '\"',\n",
       " 'big▁',\n",
       " 'air',\n",
       " '\"▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'and▁',\n",
       " 'hood,▁',\n",
       " 'with▁',\n",
       " 'holes▁',\n",
       " 'for▁',\n",
       " 'ears.▁',\n",
       " 'snow',\n",
       " 'board▁',\n",
       " 'pants,▁',\n",
       " 'with▁',\n",
       " 'hole▁',\n",
       " 'for▁',\n",
       " 'tail.▁',\n",
       " 'paw▁',\n",
       " 'print▁',\n",
       " 'arm▁',\n",
       " 'pads▁',\n",
       " 'and▁',\n",
       " 'knee▁',\n",
       " 'pads.▁',\n",
       " 'blue▁',\n",
       " 'helmet▁',\n",
       " 'skateboard▁',\n",
       " 'with▁',\n",
       " 'foot▁',\n",
       " 'holder.▁',\n",
       " 'comes▁',\n",
       " 'with▁',\n",
       " 'a▁',\n",
       " 'build▁',\n",
       " 'a▁',\n",
       " 'bear▁',\n",
       " '<unk>',\n",
       " '.▁',\n",
       " 'free▁',\n",
       " 'shipping!▁',\n",
       " 'all▁',\n",
       " 'accessories▁',\n",
       " 'from▁',\n",
       " 'build▁',\n",
       " 'a▁',\n",
       " 'bear▁',\n",
       " 'and▁',\n",
       " 'is▁',\n",
       " 'build▁',\n",
       " 'a▁',\n",
       " 'bear▁',\n",
       " 'size.▁',\n",
       " ';)▁',\n",
       " 'talk▁',\n",
       " 'to▁',\n",
       " 'me',\n",
       " ';▁',\n",
       " 'leave▁',\n",
       " 'any▁',\n",
       " 'questions▁',\n",
       " 'in▁',\n",
       " 'the▁',\n",
       " 'comments',\n",
       " '!▁',\n",
       " 'we▁',\n",
       " 'might▁',\n",
       " 'be▁',\n",
       " 'able▁',\n",
       " 'to▁',\n",
       " 'work▁',\n",
       " 'something▁',\n",
       " 'out!▁',\n",
       " 'depending▁',\n",
       " 'on▁',\n",
       " 'several▁',\n",
       " '<unk>',\n",
       " 's,▁',\n",
       " 'price▁',\n",
       " 'may▁',\n",
       " 'be▁',\n",
       " 'firm▁',\n",
       " 'or▁',\n",
       " '<unk>',\n",
       " '.▁',\n",
       " 'this▁',\n",
       " 'is▁',\n",
       " 'for▁',\n",
       " 'one▁',\n",
       " '(1)▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " 'colourpop▁',\n",
       " 'liquid▁',\n",
       " 'lipstick▁',\n",
       " 'in▁',\n",
       " 'the▁',\n",
       " 'shade▁',\n",
       " '<unk>',\n",
       " 'id',\n",
       " 'ay',\n",
       " '\".▁',\n",
       " 'it▁',\n",
       " 'is▁',\n",
       " 'a▁',\n",
       " 'true▁',\n",
       " 'black',\n",
       " '—',\n",
       " 'very▁',\n",
       " 'opa',\n",
       " 'que.▁',\n",
       " 'the▁',\n",
       " 'lipstick▁',\n",
       " 'in▁',\n",
       " 'this▁',\n",
       " 'listing▁',\n",
       " 'has▁',\n",
       " 'never▁',\n",
       " 'been▁',\n",
       " 'used.▁',\n",
       " 'it▁',\n",
       " 'has▁',\n",
       " 'not▁',\n",
       " 'been▁',\n",
       " 'swatched▁',\n",
       " 'at▁',\n",
       " 'all▁',\n",
       " 'before▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'or▁',\n",
       " 'even▁',\n",
       " 'opened.▁',\n",
       " 'the▁',\n",
       " 'only▁',\n",
       " 'time▁',\n",
       " 'it▁',\n",
       " 'was▁',\n",
       " 'taken▁',\n",
       " 'out▁',\n",
       " 'of▁',\n",
       " 'its▁',\n",
       " 'box▁',\n",
       " 'was▁',\n",
       " 'for▁',\n",
       " 'the▁',\n",
       " 'pictures.▁',\n",
       " 'i▁',\n",
       " 'already▁',\n",
       " 'have▁',\n",
       " 'one▁',\n",
       " 'that▁',\n",
       " 'i▁',\n",
       " 'use▁',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'so▁',\n",
       " 'i▁',\n",
       " 'never▁',\n",
       " 'felt▁',\n",
       " 'the▁',\n",
       " 'need▁',\n",
       " 'to▁',\n",
       " 'even▁',\n",
       " 'touch▁',\n",
       " 'this▁',\n",
       " 'one.▁',\n",
       " 'yes,▁',\n",
       " 'it▁',\n",
       " 'is▁',\n",
       " 'authentic.▁',\n",
       " 'i▁',\n",
       " 'do▁',\n",
       " 'have▁',\n",
       " 'more▁',\n",
       " 'than▁',\n",
       " 'one',\n",
       " '<unk>',\n",
       " 'ist',\n",
       " '<unk>',\n",
       " ',▁',\n",
       " 'if▁',\n",
       " 'you▁',\n",
       " 'would▁',\n",
       " 'be▁',\n",
       " 'interested▁',\n",
       " 'in▁',\n",
       " 'purchasing▁',\n",
       " 'more▁',\n",
       " 'than▁',\n",
       " 'one.▁',\n",
       " 'if▁',\n",
       " 'you▁',\n",
       " 'have▁',\n",
       " 'any▁',\n",
       " 'questions,▁',\n",
       " 'leave▁',\n",
       " 'them▁',\n",
       " 'in▁',\n",
       " 'the▁',\n",
       " 'comments▁',\n",
       " 'below!▁',\n",
       " '15▁',\n",
       " 'pink▁',\n",
       " 'bunny▁',\n",
       " 'rabbit▁',\n",
       " 'stickers▁',\n",
       " 'lot▁',\n",
       " 'this▁',\n",
       " 'listing▁',\n",
       " 'is▁',\n",
       " 'for▁',\n",
       " '15▁',\n",
       " 'random▁',\n",
       " 'stickers▁',\n",
       " 'from▁',\n",
       " 'the▁',\n",
       " 'pictures▁',\n",
       " 'shown.▁',\n",
       " 'quarter▁',\n",
       " 'in▁',\n",
       " 'pictures▁',\n",
       " 'show▁',\n",
       " 'scale▁',\n",
       " 'of▁',\n",
       " 'stickers.▁',\n",
       " 'tags:▁',\n",
       " 'bunny▁',\n",
       " 'rabbit▁',\n",
       " 'sanrio▁',\n",
       " 'stickers▁',\n",
       " 'flakes▁',\n",
       " 'lot▁',\n",
       " 'bundle▁',\n",
       " 'stationary▁',\n",
       " 'japan▁',\n",
       " 'japanese▁',\n",
       " 'cute▁',\n",
       " 'kawaii▁',\n",
       " 'korean▁',\n",
       " 'korea▁',\n",
       " 'brand▁',\n",
       " 'new▁',\n",
       " 'lanyard▁',\n",
       " 'for▁',\n",
       " 'sale▁',\n",
       " 'size:▁',\n",
       " '1▁',\n",
       " 'inch▁',\n",
       " 'width▁',\n",
       " 'x▁',\n",
       " '24▁',\n",
       " 'inches▁',\n",
       " 'long▁',\n",
       " 'free▁',\n",
       " 'shipping.',\n",
       " 'please▁',\n",
       " 'message▁',\n",
       " 'us▁',\n",
       " 'if▁',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time bpe.inverse_transform(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at encodings on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hollister jean shorts, size 9. great condition. prices are not firm check my closet and save on bund'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = bpe_text[:100].lower()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 189 µs, sys: 772 µs, total: 961 µs\n",
      "Wall time: 942 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5145, 15787,  4635,  2916,  1313,  5339,  7863, 14890, 16978,\n",
       "         761, 17894,  5248, 12414, 12835, 14416, 17428, 15183, 12793,\n",
       "       14203])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time tokens = bpe.transform(t)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hollister▁',\n",
       " 'jean▁',\n",
       " 'shorts,▁',\n",
       " 'size▁',\n",
       " '9.▁',\n",
       " 'great▁',\n",
       " 'condition.▁',\n",
       " 'prices▁',\n",
       " 'are▁',\n",
       " 'not▁',\n",
       " 'firm▁',\n",
       " 'check▁',\n",
       " 'my▁',\n",
       " 'closet▁',\n",
       " 'and▁',\n",
       " 'save▁',\n",
       " 'on▁',\n",
       " 'bund',\n",
       " '▁']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_tokens = bpe.inverse_transform(tokens)\n",
    "inv_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def make_Xy(df, tokens_per_batch, max_batch_size):\n",
    "    df['text_tokens'] = df.text.apply(bpe.transform)\n",
    "    df['n_text_tokens'] = df.text_tokens.apply(len)\n",
    "    df.sort_values('n_text_tokens', inplace=True)\n",
    "    df['batch'] = df.n_text_tokens.cumsum() % tokens_per_batch\n",
    "    def gen():\n",
    "        while True:\n",
    "            for b in df.batch:\n",
    "                batch = df[df.batch == b]\n",
    "                batch = batch.sample(frac=1)\n",
    "                for i in range(len(batch) // max_batch_size):\n",
    "                    batch_chunk = batch.iloc[i*max_batch_size:(i+1)*max_batch_size]\n",
    "                    maxlen = batch_chunk.n_text_tokens.max()\n",
    "                    X = {\n",
    "                        'category_input': batch_chunk.category_id,\n",
    "                        'brand_input': batch_chunk.brand_id,\n",
    "                        'item_condition_input': batch_chunk.item_condition_id,\n",
    "                        'text_input': pad_sequences(batch_chunk.text_tokens, maxlen=maxlen),\n",
    "                        'shipping_input': batch_chunk.shipping\n",
    "                    }\n",
    "                    y = batch_chunk.price\n",
    "                    yield X, y\n",
    "    return df, gen, df.batch.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, Xy_train_gen, train_batches = make_Xy(df_train, 64*32, 100)\n",
    "df_test, Xy_test_gen, test_batches = make_Xy(df_test, 64*32, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_input = keras.layers.Input(shape=(1,), name='category_input')\n",
    "brand_input = keras.layers.Input(shape=(1,), name='brand_input')\n",
    "item_condition_input = keras.layers.Input(shape=(1,), name='item_condition_input')\n",
    "text_input = keras.layers.Input(shape=(None,), name='text_input')\n",
    "shipping_input = keras.layers.Input(shape=(1,), name='shipping_input')\n",
    "inputs = [category_input, brand_input, item_condition_input, text_input, shipping_input]\n",
    "\n",
    "# categorical feature embeddings\n",
    "category_embedding = keras.layers.Embedding(\n",
    "    input_dim=df_train.category_id.nunique()+1,\n",
    "    output_dim=5, input_length=1)(category_input)\n",
    "\n",
    "brand_embedding = keras.layers.Embedding(\n",
    "    input_dim=df_train.brand_id.nunique()+1,\n",
    "    output_dim=5, input_length=1)(brand_input)\n",
    "\n",
    "item_condition_embedding = keras.layers.Embedding(\n",
    "    input_dim=df_train.item_condition_id.nunique()+1,\n",
    "    output_dim=5, input_length=1)(item_condition_input)\n",
    "\n",
    "embedding_tensors = [category_embedding, brand_embedding, item_condition_embedding]\n",
    "x_embeddings = keras.layers.Concatenate()([\n",
    "    keras.layers.Flatten()(embedding) for embedding in embedding_tensors\n",
    "])\n",
    "\n",
    "shipping_input = keras.layers.BatchNormalization()(shipping_input)\n",
    "\n",
    "\n",
    "# text features\n",
    "import keras.backend as K\n",
    "Sum = keras.layers.Lambda(lambda x: K.sum(x, axis=1))\n",
    "\n",
    "def SelfAttention(X):\n",
    "    dim = K.int_shape(X)[-1]\n",
    "    q = keras.layers.Dense(dim)(X)\n",
    "    q = keras.layers.Dropout(0.1)(q)    \n",
    "    k = keras.layers.Dense(dim)(X)\n",
    "    k = keras.layers.Dropout(0.1)(k)\n",
    "    v = keras.layers.Dense(dim)(X)\n",
    "    v = keras.layers.Dropout(0.1)(v)\n",
    "    w = keras.layers.Dot((2, 2))([q, k])\n",
    "    w = keras.layers.Softmax(axis=1)(w)\n",
    "    return keras.layers.Dot((2, 1))([w, v])\n",
    "    \n",
    "\n",
    "text_embeddings = keras.layers.Embedding(\n",
    "    input_dim=len(bpe.vocab)+1, output_dim=10, input_length=None)(text_input)\n",
    "text_embeddings = keras.layers.SpatialDropout1D(0.4)(text_embeddings)\n",
    "attention = SelfAttention(text_embeddings)\n",
    "x_text = Sum(attention)\n",
    "\n",
    "\n",
    "x = keras.layers.Concatenate()([x_embeddings, x_text, shipping_input])\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dense(K.int_shape(x)[-1], activation='relu')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dense(1, activation='relu')(x)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_input (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 10)     188150      text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, None, 10)     0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, None, 10)     110         spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, None, 10)     110         spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, None, 10)     0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, 10)     0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "category_input (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "brand_input (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition_input (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, None, None)   0           dropout_9[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, None, 10)     110         spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 5)         6340        category_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 5)         22260       brand_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 5)         30          item_condition_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, None, None)   0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, None, 10)     0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 5)            0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 5)            0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 5)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, None, 10)     0           softmax_3[0][0]                  \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "shipping_input (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 15)           0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10)           0           dot_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1)            4           shipping_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 26)           0           concatenate_5[0][0]              \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 26)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 26)           104         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 26)           702         batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 26)           104         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            27          batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 218,051\n",
      "Trainable params: 217,945\n",
      "Non-trainable params: 106\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1067pt\" viewBox=\"0.00 0.00 1021.79 1067.00\" width=\"1022pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1063)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1063 1017.79,-1063 1017.79,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4607918544 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4607918544</title>\n",
       "<polygon fill=\"none\" points=\"141.624,-1022.5 141.624,-1058.5 283.9795,-1058.5 283.9795,-1022.5 141.624,-1022.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.8018\" y=\"-1036.3\">text_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5989083848 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5989083848</title>\n",
       "<polygon fill=\"none\" points=\"127.231,-949.5 127.231,-985.5 298.3726,-985.5 298.3726,-949.5 127.231,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.8018\" y=\"-963.3\">embedding_12: Embedding</text>\n",
       "</g>\n",
       "<!-- 4607918544&#45;&gt;5989083848 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4607918544-&gt;5989083848</title>\n",
       "<path d=\"M212.8018,-1022.4551C212.8018,-1014.3828 212.8018,-1004.6764 212.8018,-995.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.3019,-995.5903 212.8018,-985.5904 209.3019,-995.5904 216.3019,-995.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5723061888 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5723061888</title>\n",
       "<polygon fill=\"none\" points=\"92.231,-876.5 92.231,-912.5 333.3726,-912.5 333.3726,-876.5 92.231,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.8018\" y=\"-890.3\">spatial_dropout1d_3: SpatialDropout1D</text>\n",
       "</g>\n",
       "<!-- 5989083848&#45;&gt;5723061888 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5989083848-&gt;5723061888</title>\n",
       "<path d=\"M212.8018,-949.4551C212.8018,-941.3828 212.8018,-931.6764 212.8018,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.3019,-922.5903 212.8018,-912.5904 209.3019,-922.5904 216.3019,-922.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4611468872 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4611468872</title>\n",
       "<polygon fill=\"none\" points=\"18.4321,-803.5 18.4321,-839.5 129.1714,-839.5 129.1714,-803.5 18.4321,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"73.8018\" y=\"-817.3\">dense_11: Dense</text>\n",
       "</g>\n",
       "<!-- 5723061888&#45;&gt;4611468872 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5723061888-&gt;4611468872</title>\n",
       "<path d=\"M178.4422,-876.4551C159.981,-866.7596 137.0308,-854.7066 117.3317,-844.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"118.7285,-841.1413 108.2478,-839.5904 115.4737,-847.3387 118.7285,-841.1413\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5346967056 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5346967056</title>\n",
       "<polygon fill=\"none\" points=\"157.1758,-803.5 157.1758,-839.5 268.4277,-839.5 268.4277,-803.5 157.1758,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.8018\" y=\"-817.3\">dense_12: Dense</text>\n",
       "</g>\n",
       "<!-- 5723061888&#45;&gt;5346967056 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5723061888-&gt;5346967056</title>\n",
       "<path d=\"M212.8018,-876.4551C212.8018,-868.3828 212.8018,-858.6764 212.8018,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.3019,-849.5903 212.8018,-839.5904 209.3019,-849.5904 216.3019,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347270608 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>5347270608</title>\n",
       "<polygon fill=\"none\" points=\"297.1758,-803.5 297.1758,-839.5 408.4277,-839.5 408.4277,-803.5 297.1758,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"352.8018\" y=\"-817.3\">dense_13: Dense</text>\n",
       "</g>\n",
       "<!-- 5723061888&#45;&gt;5347270608 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>5723061888-&gt;5347270608</title>\n",
       "<path d=\"M247.4085,-876.4551C266.0866,-866.7157 289.3271,-854.5975 309.2278,-844.2207\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"310.8592,-847.3174 318.1079,-839.5904 307.6227,-841.1105 310.8592,-847.3174\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5346964984 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5346964984</title>\n",
       "<polygon fill=\"none\" points=\"0,-730.5 0,-766.5 127.6035,-766.5 127.6035,-730.5 0,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63.8018\" y=\"-744.3\">dropout_9: Dropout</text>\n",
       "</g>\n",
       "<!-- 4611468872&#45;&gt;5346964984 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>4611468872-&gt;5346964984</title>\n",
       "<path d=\"M71.3298,-803.4551C70.2241,-795.3828 68.8944,-785.6764 67.6623,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"71.1048,-776.0228 66.2799,-766.5904 64.1695,-776.9729 71.1048,-776.0228\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347179992 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5347179992</title>\n",
       "<polygon fill=\"none\" points=\"145.5,-730.5 145.5,-766.5 280.1035,-766.5 280.1035,-730.5 145.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.8018\" y=\"-744.3\">dropout_10: Dropout</text>\n",
       "</g>\n",
       "<!-- 5346967056&#45;&gt;5347179992 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5346967056-&gt;5347179992</title>\n",
       "<path d=\"M212.8018,-803.4551C212.8018,-795.3828 212.8018,-785.6764 212.8018,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.3019,-776.5903 212.8018,-766.5904 209.3019,-776.5904 216.3019,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347383616 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>5347383616</title>\n",
       "<polygon fill=\"none\" points=\"174.6621,-657.5 174.6621,-693.5 250.9414,-693.5 250.9414,-657.5 174.6621,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"212.8018\" y=\"-671.3\">dot_5: Dot</text>\n",
       "</g>\n",
       "<!-- 5346964984&#45;&gt;5347383616 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5346964984-&gt;5347383616</title>\n",
       "<path d=\"M100.6332,-730.4551C120.6016,-720.6719 145.4696,-708.4883 166.7126,-698.0806\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"168.4374,-701.1331 175.8776,-693.5904 165.3575,-694.847 168.4374,-701.1331\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347179992&#45;&gt;5347383616 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5347179992-&gt;5347383616</title>\n",
       "<path d=\"M212.8018,-730.4551C212.8018,-722.3828 212.8018,-712.6764 212.8018,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"216.3019,-703.5903 212.8018,-693.5904 209.3019,-703.5904 216.3019,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 6012569080 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>6012569080</title>\n",
       "<polygon fill=\"none\" points=\"378.0239,-657.5 378.0239,-693.5 547.5796,-693.5 547.5796,-657.5 378.0239,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"462.8018\" y=\"-671.3\">category_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4607920168 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>4607920168</title>\n",
       "<polygon fill=\"none\" points=\"378.731,-584.5 378.731,-620.5 542.8726,-620.5 542.8726,-584.5 378.731,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"460.8018\" y=\"-598.3\">embedding_9: Embedding</text>\n",
       "</g>\n",
       "<!-- 6012569080&#45;&gt;4607920168 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>6012569080-&gt;4607920168</title>\n",
       "<path d=\"M462.3074,-657.4551C462.0862,-649.3828 461.8203,-639.6764 461.5739,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"465.07,-630.4907 461.2974,-620.5904 458.0727,-630.6825 465.07,-630.4907\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 6012570144 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>6012570144</title>\n",
       "<polygon fill=\"none\" points=\"568.1826,-657.5 568.1826,-693.5 721.4209,-693.5 721.4209,-657.5 568.1826,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"644.8018\" y=\"-671.3\">brand_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5361751768 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>5361751768</title>\n",
       "<polygon fill=\"none\" points=\"561.231,-584.5 561.231,-620.5 732.3726,-620.5 732.3726,-584.5 561.231,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"646.8018\" y=\"-598.3\">embedding_10: Embedding</text>\n",
       "</g>\n",
       "<!-- 6012570144&#45;&gt;5361751768 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>6012570144-&gt;5361751768</title>\n",
       "<path d=\"M645.2961,-657.4551C645.5173,-649.3828 645.7832,-639.6764 646.0297,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"649.5309,-630.6825 646.3061,-620.5904 642.5335,-630.4907 649.5309,-630.6825\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4607921680 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>4607921680</title>\n",
       "<polygon fill=\"none\" points=\"739.7378,-657.5 739.7378,-693.5 945.8657,-693.5 945.8657,-657.5 739.7378,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"842.8018\" y=\"-671.3\">item_condition_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5989084408 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>5989084408</title>\n",
       "<polygon fill=\"none\" points=\"754.4873,-584.5 754.4873,-620.5 925.1162,-620.5 925.1162,-584.5 754.4873,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839.8018\" y=\"-598.3\">embedding_11: Embedding</text>\n",
       "</g>\n",
       "<!-- 4607921680&#45;&gt;5989084408 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>4607921680-&gt;5989084408</title>\n",
       "<path d=\"M842.0602,-657.4551C841.7284,-649.3828 841.3296,-639.6764 840.9599,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"844.453,-630.4382 840.5452,-620.5904 837.4589,-630.7257 844.453,-630.4382\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347301752 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>5347301752</title>\n",
       "<polygon fill=\"none\" points=\"176.8345,-584.5 176.8345,-620.5 306.769,-620.5 306.769,-584.5 176.8345,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.8018\" y=\"-598.3\">softmax_3: Softmax</text>\n",
       "</g>\n",
       "<!-- 5347383616&#45;&gt;5347301752 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>5347383616-&gt;5347301752</title>\n",
       "<path d=\"M219.9703,-657.4551C223.2468,-649.2074 227.201,-639.2536 230.8389,-630.0962\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"234.1759,-631.1761 234.6152,-620.5904 227.6704,-628.5917 234.1759,-631.1761\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347302592 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>5347302592</title>\n",
       "<polygon fill=\"none\" points=\"297.7563,-730.5 297.7563,-766.5 431.8472,-766.5 431.8472,-730.5 297.7563,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"364.8018\" y=\"-744.3\">dropout_11: Dropout</text>\n",
       "</g>\n",
       "<!-- 5347270608&#45;&gt;5347302592 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>5347270608-&gt;5347302592</title>\n",
       "<path d=\"M355.7681,-803.4551C357.095,-795.3828 358.6906,-785.6764 360.1691,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"363.6595,-777.0257 361.828,-766.5904 356.7522,-775.8902 363.6595,-777.0257\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5929228496 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>5929228496</title>\n",
       "<polygon fill=\"none\" points=\"406.1689,-511.5 406.1689,-547.5 517.4346,-547.5 517.4346,-511.5 406.1689,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.8018\" y=\"-525.3\">flatten_7: Flatten</text>\n",
       "</g>\n",
       "<!-- 4607920168&#45;&gt;5929228496 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>4607920168-&gt;5929228496</title>\n",
       "<path d=\"M461.0489,-584.4551C461.1595,-576.3828 461.2925,-566.6764 461.4157,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"464.9165,-557.6374 461.5539,-547.5904 457.9172,-557.5414 464.9165,-557.6374\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5590697072 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>5590697072</title>\n",
       "<polygon fill=\"none\" points=\"563.1689,-511.5 563.1689,-547.5 674.4346,-547.5 674.4346,-511.5 563.1689,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-525.3\">flatten_8: Flatten</text>\n",
       "</g>\n",
       "<!-- 5361751768&#45;&gt;5590697072 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>5361751768-&gt;5590697072</title>\n",
       "<path d=\"M639.8804,-584.4551C636.7169,-576.2074 632.899,-566.2536 629.3866,-557.0962\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"632.5897,-555.6737 625.7405,-547.5904 626.0539,-558.1806 632.5897,-555.6737\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4578416736 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>4578416736</title>\n",
       "<polygon fill=\"none\" points=\"692.1689,-511.5 692.1689,-547.5 803.4346,-547.5 803.4346,-511.5 692.1689,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"747.8018\" y=\"-525.3\">flatten_9: Flatten</text>\n",
       "</g>\n",
       "<!-- 5989084408&#45;&gt;4578416736 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>5989084408-&gt;4578416736</title>\n",
       "<path d=\"M817.0602,-584.4551C805.4495,-575.2422 791.156,-563.9006 778.5752,-553.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"780.6097,-551.0644 770.6006,-547.5904 776.2586,-556.5479 780.6097,-551.0644\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347302424 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>5347302424</title>\n",
       "<polygon fill=\"none\" points=\"311.6621,-511.5 311.6621,-547.5 387.9414,-547.5 387.9414,-511.5 311.6621,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"349.8018\" y=\"-525.3\">dot_6: Dot</text>\n",
       "</g>\n",
       "<!-- 5347301752&#45;&gt;5347302424 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>5347301752-&gt;5347302424</title>\n",
       "<path d=\"M268.4984,-584.4551C282.388,-575.0667 299.5481,-563.4678 314.5181,-553.3491\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"316.713,-556.0901 323.0379,-547.5904 312.793,-550.2907 316.713,-556.0901\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347302592&#45;&gt;5347302424 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>5347302592-&gt;5347302424</title>\n",
       "<path d=\"M363.5487,-730.2049C360.9282,-691.9458 354.854,-603.2623 351.7315,-557.6742\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"355.216,-557.3265 351.0407,-547.589 348.2323,-557.8049 355.216,-557.3265\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5983536856 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>5983536856</title>\n",
       "<polygon fill=\"none\" points=\"532.4824,-438.5 532.4824,-474.5 705.1211,-474.5 705.1211,-438.5 532.4824,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-452.3\">concatenate_5: Concatenate</text>\n",
       "</g>\n",
       "<!-- 5929228496&#45;&gt;5983536856 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>5929228496-&gt;5983536856</title>\n",
       "<path d=\"M500.6108,-511.4551C521.7456,-501.628 548.0895,-489.3789 570.5389,-478.9407\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"572.3031,-481.9803 579.8951,-474.5904 569.3517,-475.6329 572.3031,-481.9803\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5590697072&#45;&gt;5983536856 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>5590697072-&gt;5983536856</title>\n",
       "<path d=\"M618.8018,-511.4551C618.8018,-503.3828 618.8018,-493.6764 618.8018,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-484.5903 618.8018,-474.5904 615.3019,-484.5904 622.3019,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4578416736&#45;&gt;5983536856 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>4578416736-&gt;5983536856</title>\n",
       "<path d=\"M715.9141,-511.4551C699.0137,-501.8912 678.0594,-490.0334 659.9462,-479.7833\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"661.1966,-476.4693 650.7697,-474.5904 657.749,-482.5615 661.1966,-476.4693\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5921126552 -->\n",
       "<g class=\"node\" id=\"node24\">\n",
       "<title>5921126552</title>\n",
       "<polygon fill=\"none\" points=\"352.5688,-438.5 352.5688,-474.5 477.0347,-474.5 477.0347,-438.5 352.5688,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414.8018\" y=\"-452.3\">lambda_3: Lambda</text>\n",
       "</g>\n",
       "<!-- 5347302424&#45;&gt;5921126552 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>5347302424-&gt;5921126552</title>\n",
       "<path d=\"M365.8692,-511.4551C373.7599,-502.5932 383.4044,-491.7616 392.0387,-482.0646\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"394.6578,-484.3863 398.6939,-474.5904 389.4299,-479.7313 394.6578,-484.3863\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4607920784 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>4607920784</title>\n",
       "<polygon fill=\"none\" points=\"821.0068,-511.5 821.0068,-547.5 990.5967,-547.5 990.5967,-511.5 821.0068,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"905.8018\" y=\"-525.3\">shipping_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5983538144 -->\n",
       "<g class=\"node\" id=\"node25\">\n",
       "<title>5983538144</title>\n",
       "<polygon fill=\"none\" points=\"747.8135,-438.5 747.8135,-474.5 1013.79,-474.5 1013.79,-438.5 747.8135,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880.8018\" y=\"-452.3\">batch_normalization_6: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 4607920784&#45;&gt;5983538144 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>4607920784-&gt;5983538144</title>\n",
       "<path d=\"M899.622,-511.4551C896.7974,-503.2074 893.3886,-493.2536 890.2525,-484.0962\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"893.5483,-482.917 886.9971,-474.5904 886.9259,-485.185 893.5483,-482.917\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347354608 -->\n",
       "<g class=\"node\" id=\"node26\">\n",
       "<title>5347354608</title>\n",
       "<polygon fill=\"none\" points=\"532.4824,-365.5 532.4824,-401.5 705.1211,-401.5 705.1211,-365.5 532.4824,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-379.3\">concatenate_6: Concatenate</text>\n",
       "</g>\n",
       "<!-- 5983536856&#45;&gt;5347354608 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>5983536856-&gt;5347354608</title>\n",
       "<path d=\"M618.8018,-438.4551C618.8018,-430.3828 618.8018,-420.6764 618.8018,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-411.5903 618.8018,-401.5904 615.3019,-411.5904 622.3019,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5921126552&#45;&gt;5347354608 -->\n",
       "<g class=\"edge\" id=\"edge26\">\n",
       "<title>5921126552-&gt;5347354608</title>\n",
       "<path d=\"M465.2287,-438.4551C493.5488,-428.3209 529.0669,-415.611 558.8103,-404.9675\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"560.0118,-408.255 568.2479,-401.5904 557.6533,-401.6643 560.0118,-408.255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5983538144&#45;&gt;5347354608 -->\n",
       "<g class=\"edge\" id=\"edge27\">\n",
       "<title>5983538144-&gt;5347354608</title>\n",
       "<path d=\"M816.0377,-438.4551C778.8786,-428.1016 732.0704,-415.0596 693.3797,-404.2794\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"694.3013,-400.9029 683.7288,-401.5904 692.4224,-407.646 694.3013,-400.9029\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5360346224 -->\n",
       "<g class=\"node\" id=\"node27\">\n",
       "<title>5360346224</title>\n",
       "<polygon fill=\"none\" points=\"551.5,-292.5 551.5,-328.5 686.1035,-328.5 686.1035,-292.5 551.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-306.3\">dropout_12: Dropout</text>\n",
       "</g>\n",
       "<!-- 5347354608&#45;&gt;5360346224 -->\n",
       "<g class=\"edge\" id=\"edge28\">\n",
       "<title>5347354608-&gt;5360346224</title>\n",
       "<path d=\"M618.8018,-365.4551C618.8018,-357.3828 618.8018,-347.6764 618.8018,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-338.5903 618.8018,-328.5904 615.3019,-338.5904 622.3019,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347503912 -->\n",
       "<g class=\"node\" id=\"node28\">\n",
       "<title>5347503912</title>\n",
       "<polygon fill=\"none\" points=\"485.8135,-219.5 485.8135,-255.5 751.79,-255.5 751.79,-219.5 485.8135,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-233.3\">batch_normalization_7: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 5360346224&#45;&gt;5347503912 -->\n",
       "<g class=\"edge\" id=\"edge29\">\n",
       "<title>5360346224-&gt;5347503912</title>\n",
       "<path d=\"M618.8018,-292.4551C618.8018,-284.3828 618.8018,-274.6764 618.8018,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-265.5903 618.8018,-255.5904 615.3019,-265.5904 622.3019,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347441408 -->\n",
       "<g class=\"node\" id=\"node29\">\n",
       "<title>5347441408</title>\n",
       "<polygon fill=\"none\" points=\"563.1758,-146.5 563.1758,-182.5 674.4277,-182.5 674.4277,-146.5 563.1758,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-160.3\">dense_14: Dense</text>\n",
       "</g>\n",
       "<!-- 5347503912&#45;&gt;5347441408 -->\n",
       "<g class=\"edge\" id=\"edge30\">\n",
       "<title>5347503912-&gt;5347441408</title>\n",
       "<path d=\"M618.8018,-219.4551C618.8018,-211.3828 618.8018,-201.6764 618.8018,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-192.5903 618.8018,-182.5904 615.3019,-192.5904 622.3019,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347442304 -->\n",
       "<g class=\"node\" id=\"node30\">\n",
       "<title>5347442304</title>\n",
       "<polygon fill=\"none\" points=\"485.8135,-73.5 485.8135,-109.5 751.79,-109.5 751.79,-73.5 485.8135,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-87.3\">batch_normalization_8: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 5347441408&#45;&gt;5347442304 -->\n",
       "<g class=\"edge\" id=\"edge31\">\n",
       "<title>5347441408-&gt;5347442304</title>\n",
       "<path d=\"M618.8018,-146.4551C618.8018,-138.3828 618.8018,-128.6764 618.8018,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-119.5903 618.8018,-109.5904 615.3019,-119.5904 622.3019,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5347620008 -->\n",
       "<g class=\"node\" id=\"node31\">\n",
       "<title>5347620008</title>\n",
       "<polygon fill=\"none\" points=\"563.1758,-.5 563.1758,-36.5 674.4277,-36.5 674.4277,-.5 563.1758,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618.8018\" y=\"-14.3\">dense_15: Dense</text>\n",
       "</g>\n",
       "<!-- 5347442304&#45;&gt;5347620008 -->\n",
       "<g class=\"edge\" id=\"edge32\">\n",
       "<title>5347442304-&gt;5347620008</title>\n",
       "<path d=\"M618.8018,-73.4551C618.8018,-65.3828 618.8018,-55.6764 618.8018,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.3019,-46.5903 618.8018,-36.5904 615.3019,-46.5904 622.3019,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.log(y_pred+1.) - K.log(y_true+1.))))\n",
    "model.compile(loss=rmsle, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2048/2048 [==============================] - 1822s 890ms/step - loss: 0.9499 - val_loss: 0.8468\n",
      "Epoch 2/10\n",
      "2048/2048 [==============================] - 1779s 869ms/step - loss: 0.5889 - val_loss: 0.8321\n",
      "Epoch 3/10\n",
      "2048/2048 [==============================] - 1773s 866ms/step - loss: 0.5629 - val_loss: 0.8076\n",
      "Epoch 4/10\n",
      "2048/2048 [==============================] - 1832s 894ms/step - loss: 0.5505 - val_loss: 0.7671\n",
      "Epoch 5/10\n",
      "1406/2048 [===================>..........] - ETA: 8:03 - loss: 0.5462"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    Xy_train_gen(),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_batches,\n",
    "    validation_data=Xy_test_gen(),\n",
    "    validation_steps=test_batches,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
    "               keras.callbacks.EarlyStopping(patience=3),\n",
    "               keras.callbacks.TerminateOnNaN()]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
